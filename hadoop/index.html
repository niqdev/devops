<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="niqdev">
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Hadoop - DevOps</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../css/theme.css" />
  <link rel="stylesheet" href="../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Hadoop";
    var mkdocs_page_input_path = "hadoop.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../js/jquery-2.1.1.min.js" defer></script>
  <script src="../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-68888222-4', 'niqdev.github.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> DevOps</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../linux/">Linux</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../docker/">Docker</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../ansible/">Ansible</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../cassandra/">Cassandra</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../zookeeper/">ZooKeeper</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../kafka/">Kafka</a>
                    </li>
                </ul>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Hadoop</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hdfs-and-mapreduce">HDFS and MapReduce</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#admin">Admin</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mapreduce-wordcount-job">MapReduce WordCount Job</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#benchmarking-mapreduce-with-terasort">Benchmarking MapReduce with TeraSort</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#spark">Spark</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#interactive-analysis-example">Interactive Analysis example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#spark-job-examples">Spark Job examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#zeppelin">Zeppelin</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setup_1">Setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples">Examples</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#oozie">Oozie</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setup_2">Setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#examples_1">Examples</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#useful-commands">Useful commands</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../cloud/">Cloud</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../kubernetes/">Kubernetes</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../system-design/">System Design</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../operating-system/">Operating System</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../programming/">Programming</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../other-resources/">Other Resources</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../toolbox/">Toolbox</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../jvm/">JVM (OLD)</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../scala/">Scala (OLD)</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">DevOps</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Hadoop</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/niqdev/devops/edit/master/docs/hadoop.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="hadoop">Hadoop</h1>
<p>The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the <a href="https://github.com/niqdev/devops/blob/master/hadoop/Vagrantfile">Vagrantfile</a> and the Vagrant <a href="other/#vagrant">guide</a> for more details.</p>
<p>Resources</p>
<ul>
<li>
<p><a href="https://hadoop.apache.org">Documentation</a></p>
</li>
<li>
<p><a href="https://amzn.to/2Kxc8bg">Hadoop: The Definitive Guide</a> (2015)(4th) by Tom White (Book)</p>
</li>
<li>
<p><a href="https://hadoopecosystemtable.github.io">The Hadoop Ecosystem Table</a></p>
</li>
<li>
<p><a href="https://ercoppa.github.io/HadoopInternals">Hadoop Internals</a></p>
</li>
</ul>
<h3 id="setup">Setup</h3>
<p>Requirements</p>
<ul>
<li><a href="https://www.vagrantup.com">Vagrant</a></li>
<li><a href="https://www.virtualbox.org">VirtualBox</a></li>
</ul>
<p>Directory structure</p>
<pre><code class="bash">tree -a hadoop/
hadoop/
├── .data # mounted volume
│   ├── hadoop_rsa
│   ├── hadoop_rsa.pub
│   ├── master
│   │   ├── hadoop
│   │   │   ├── log
│   │   │   │   ├── hadoop
│   │   │   │   ├── mapred
│   │   │   │   └── yarn
│   │   │   ├── namenode
│   │   │   └── secondary
│   │   ├── oozie
│   │   │   ├── data
│   │   │   └── log
│   │   ├── spark
│   │   │   └── log
│   │   └── zeppelin
│   │       ├── log
│   │       └── notebook
│   ├── node-1
│   │   └── hadoop
│   │       ├── datanode
│   │       └── log
│   │           ├── hadoop
│   │           ├── mapred
│   │           └── yarn
│   ├── node-2
│   ├── node-3
├── example
│   ├── map-reduce
│   └── spark
├── file
│   ├── hadoop
│   │   ├── config
│   │   │   ├── core-site.xml
│   │   │   ├── fair-scheduler.xml
│   │   │   ├── hdfs-site.xml
│   │   │   ├── mapred-site.xml
│   │   │   ├── masters
│   │   │   ├── slaves
│   │   │   └── yarn-site.xml
│   │   └── profile-hadoop.sh
│   ├── hosts
│   ├── motd
│   ├── oozie
│   │   ├── config
│   │   │   ├── oozie-env.sh
│   │   │   └── oozie-site.xml
│   │   └── profile-oozie.sh
│   ├── spark
│   │   ├── config
│   │   │   ├── log4j.properties
│   │   │   └── spark-env.sh
│   │   └── profile-spark.sh
│   ├── ssh
│   │   └── config
│   └── zeppelin
│       ├── config
│       │   └── zeppelin-env.sh
│       └── profile-zeppelin.sh
├── script
│   ├── bootstrap.sh
│   ├── setup_hadoop.sh
│   ├── setup_oozie.sh
│   ├── setup_spark.sh
│   ├── setup_ubuntu.sh
│   └── setup_zeppelin.sh
├── Vagrantfile
└── vagrant_hadoop.sh
</code></pre>

<p>Import the script</p>
<pre><code class="bash">source vagrant_hadoop.sh
</code></pre>

<p>Create and start a Multi Node Hadoop Cluster</p>
<pre><code class="bash">hadoop-start
</code></pre>

<p><em>The first time it might take a while</em></p>
<p>Access the cluster via ssh, check also the <a href="https://github.com/niqdev/devops/blob/master/hadoop/file/hosts">/etc/hosts</a> file</p>
<pre><code class="bash">vagrant ssh master
ssh hadoop@172.16.0.10 -i .data/hadoop_rsa

# 3 nodes
vagrant ssh node-1
ssh hadoop@172.16.0.101 -i .data/hadoop_rsa
</code></pre>

<p>Destroy the cluster</p>
<pre><code class="bash">hadoop-destroy
</code></pre>

<p>For convenience add to the host machine</p>
<pre><code class="bash">cat hadoop/file/hosts | sudo tee --append /etc/hosts
</code></pre>

<p>Web UI links</p>
<ul>
<li>NameNode: <a href="http://172.16.0.10:50070">http://namenode.local:50070</a></li>
<li>NameNode metrics: <a href="http://172.16.0.10:50070/jmx">http://namenode.local:50070/jmx</a></li>
<li>ResourceManager: <a href="http://172.16.0.10:8088">http://resource-manager.local:8088</a></li>
<li>Log Level: <a href="http://172.16.0.10:8088/logLevel">http://resource-manager.local:8088/logLevel</a></li>
<li>Web Application Proxy Server: <a href="http://172.16.0.10:8100/proxy/application_XXX_0000">http://web-proxy.local:8100/proxy/application_XXX_0000</a></li>
<li>MapReduce Job History Server: <a href="http://172.16.0.10:19888">http://history.local:19888</a></li>
<li>DataNode/NodeManager (1): <a href="http://172.16.0.101:8042/node">http://node-1.local:8042/node</a></li>
<li>DataNode/NodeManager (2): <a href="http://172.16.0.102:8042/node">http://node-2.local:8042/node</a></li>
<li>DataNode/NodeManager (3): <a href="http://172.16.0.103:8042/node">http://node-3.local:8042/node</a></li>
<li>Spark: <a href="http://172.16.0.10:4040">http://spark.local:4040</a></li>
<li>Spark History Server: <a href="http://172.16.0.10:18080">http://spark-history.local:18080</a></li>
<li>Zeppelin (*): <a href="http://172.16.0.10:8080">http://zeppelin.local:8080</a></li>
<li>Oozie (*): <a href="http://172.16.0.10:11000">http://oozie.local:11000</a></li>
</ul>
<p><em>(*) Not installed by default</em></p>
<h2 id="hdfs-and-mapreduce">HDFS and MapReduce</h2>
<blockquote>
<p><strong>HDFS</strong> is a distributed file system that provides high-throughput access to application data</p>
<p><strong>YARN</strong> is a framework for job scheduling and cluster resource management</p>
<p><strong>MapReduce</strong> is a YARN-based system for parallel processing of large data sets</p>
</blockquote>
<p>Documentation</p>
<ul>
<li><a href="http://hadoop.apache.org/docs/r2.7.6">Hadoop v2.7.6</a></li>
<li><a href="http://blog.cloudera.com/blog/2015/09/untangling-apache-hadoop-yarn-part-1/">Untangling Apache Hadoop YARN</a> series</li>
</ul>
<h3 id="admin">Admin</h3>
<p>HDFS cli</p>
<pre><code class="bash"># help
hdfs

# filesystem statistics
hdfs dfsadmin -report

# filesystem check
hdfs fsck /
</code></pre>

<p>YARN cli</p>
<pre><code class="bash"># help
yarn

# list yarn applications
yarn application -list

# list nodes
yarn node -list

# view application logs
yarn logs -applicationId APPLICATION_ID

# kill yarn application
yarn application -kill APPLICATION_ID
</code></pre>

<p>Useful paths</p>
<pre><code class="bash"># data and logs
devops/hadoop/.data/master/hadoop # host
/vol/hadoop # guest

# (guest) config
/usr/local/hadoop/etc/hadoop

# (hdfs) map-reduce history
/mr-history/history/done_intermediate/hadoop

# (hdfs) aggregated app logs
/yarn/app/hadoop/logs/application_XXX
</code></pre>

<h3 id="mapreduce-wordcount-job">MapReduce WordCount Job</h3>
<pre><code class="bash"># build jar on the host machine
cd devops/hadoop/example/map-reduce
./gradlew clean build

cd devops/hadoop
vagrant ssh master

# create base directory using hdfs
hdfs dfs -mkdir -p /user/ubuntu

# create example directory
hadoop fs -mkdir -p /user/ubuntu/word-count/input

# list directory
hadoop fs -ls -h -R /
hadoop fs -ls -h -R /user/ubuntu

# create sample files
echo &quot;Hello World Bye World&quot; &gt; file01
echo &quot;Hello Hadoop Goodbye Hadoop&quot; &gt; file02

# copy from local to hdfs
hadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input
hadoop fs -put file02 /user/ubuntu/word-count/input

# verify copied files
hadoop fs -ls -h -R /user/ubuntu
hadoop fs -cat /user/ubuntu/word-count/input/file01
hadoop fs -cat /user/ubuntu/word-count/input/file02
hadoop fs -cat /user/ubuntu/word-count/input/*

# run application
hadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \
  /user/ubuntu/word-count/input \
  /user/ubuntu/word-count/output

# check output
hadoop fs -cat /user/ubuntu/word-count/output/part-r-00000

# delete directory to run it again
hadoop fs -rm -R /user/ubuntu/word-count/output

# run sample job in a different queue
hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  wordcount \
  -Dmapreduce.job.queuename=root.priority_queue \
  /user/ubuntu/word-count/input \
  /user/ubuntu/word-count/output

# well known WARN issue
# https://issues.apache.org/jira/browse/HDFS-10429
</code></pre>

<h3 id="benchmarking-mapreduce-with-terasort">Benchmarking MapReduce with TeraSort</h3>
<pre><code class="bash"># generate random data
hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  teragen 1000 random-data

# run terasort benchmark
hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  terasort random-data sorted-data

# validate data
hadoop jar \
  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \
  teravalidate sorted-data report

# useful commands
hadoop fs -ls -h -R .
hadoop fs -rm -r random-data
hadoop fs -cat random-data/part-m-00000
hadoop fs -cat sorted-data/part-r-00000
</code></pre>

<p><br></p>
<h2 id="spark">Spark</h2>
<blockquote>
<p><strong>Spark</strong> is an open-source cluster-computing framework</p>
</blockquote>
<p>Resources</p>
<ul>
<li>
<p><a href="https://spark.apache.org/docs/latest">Documentation</a></p>
</li>
<li>
<p><a href="https://amzn.to/2MzgHio">Spark in Action</a> (2016) by Petar Zečević and Marko Bonaći (Book)</p>
</li>
<li>
<p><a href="https://www.coursera.org/learn/scala-spark-big-data">Big Data Analysis with Scala and Spark</a> (Course)</p>
</li>
<li>
<p><a href="http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1">How-to: Tune Your Apache Spark Jobs</a> series</p>
</li>
<li>
<p><a href="http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application">Understanding Resource Allocation configurations for a Spark application</a></p>
</li>
<li>
<p><a href="http://c2fo.io/c2fo/spark/aws/emr/2016/07/06/apache-spark-config-cheatsheet">Apache Spark: Config Cheatsheet</a></p>
</li>
<li>
<p><a href="https://legacy.gitbook.com/book/jaceklaskowski/mastering-apache-spark">Mastering Apache Spark</a></p>
</li>
<li>
<p><a href="https://hackernoon.com/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4">Managing Spark Partitions with Coalesce and Repartition</a></p>
</li>
<li>
<p><a href="http://sujithjay.com/2018/07/24/Understanding-Apache-Spark-on-YARN/">Understanding Apache Spark on YARN</a></p>
</li>
</ul>
<p><img alt="spark-architecture" src="../img/spark-architecture.png" /></p>
<p>Spark application on YARN</p>
<p><img alt="spark-job" src="../img/spark-job.png" /></p>
<pre><code class="bash"># start REPL
spark-shell
pyspark
</code></pre>

<h3 id="interactive-analysis-example">Interactive Analysis example</h3>
<pre><code class="bash">spark-shell
# spark shell with yarn
spark-shell --master yarn --deploy-mode client

# view all configured parameters
sc.getConf.getAll.foreach(x =&gt; println(s&quot;${x._1}: ${x._2}&quot;))

val licenceLines = sc.textFile(&quot;file:/usr/local/spark/LICENSE&quot;)
val lineCount = licenceLines.count
val isBsd = (line: String) =&gt; line.contains(&quot;BSD&quot;)
val bsdLines = licenceLines.filter(isBsd)
bsdLines.count
bsdLines.foreach(println)
</code></pre>

<h3 id="spark-job-examples">Spark Job examples</h3>
<p>Example local</p>
<pre><code class="bash"># run SparkPi example
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master local[*] \
  $SPARK_HOME/examples/jars/spark-examples_*.jar 10

# GitHub event documentation
# https://developer.github.com/v3/activity/events/types

# build jar on the host machine
cd devops/hadoop/example/spark
sbt clean package

cd devops/hadoop
vagrant ssh master

# sample dataset
mkdir -p github-archive &amp;&amp; \
  cd $_ &amp;&amp; \
  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz &amp;&amp; \
  gunzip -k *
# sample line
head -n 1 2018-01-01-0.json | jq '.'

# run local job
spark-submit \
  --class &quot;com.github.niqdev.App&quot; \
  --master local[*] \
  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar
</code></pre>

<p>Example cluster</p>
<pre><code class="bash"># run job in YARN cluster-deploy mode
spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 2g \
  --executor-memory 1g \
  --executor-cores 3 \
  --queue default \
  $SPARK_HOME/examples/jars/spark-examples*.jar \
  10

# --conf &quot;spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar&quot;
</code></pre>

<p><br></p>
<h2 id="zeppelin">Zeppelin</h2>
<blockquote>
<p><strong>Zeppelin</strong> is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more</p>
</blockquote>
<p>Resources</p>
<ul>
<li><a href="https://zeppelin.apache.org">Documentation</a></li>
</ul>
<h3 id="setup_1">Setup</h3>
<p>Install and start Zeppelin</p>
<pre><code class="bash"># access master node
vagrant ssh master

# login as root
sudo su -

# install and init
/vagrant/script/setup_zeppelin.sh

# start manually (first time only)
su --login hadoop /vagrant/script/bootstrap.sh zeppelin
</code></pre>

<h3 id="examples">Examples</h3>
<ul>
<li><a href="https://hortonworks.com/tutorial/learning-spark-sql-with-zeppelin">Learning Spark SQL with Zeppelin</a></li>
</ul>
<pre><code># markdown interpreter
%md
hello

# shell interpreter
%sh
hadoop fs -ls -h -R /
</code></pre>

<p>Cluster issue: verify to have enough memory with <code>free -m</code> e.g. <em>Error: Cannot allocate memory</em></p>
<p><br></p>
<h2 id="oozie">Oozie</h2>
<blockquote>
<p><strong>Oozie</strong> is a workflow scheduler system to manage Hadoop jobs</p>
</blockquote>
<p>Resources</p>
<ul>
<li><a href="https://oozie.apache.org">Documentation</a></li>
</ul>
<h3 id="setup_2">Setup</h3>
<p><strong>Optional PostgreSQL configuration</strong> - By default Oozie is configured to use Embedded Derby</p>
<pre><code class="bash"># access master node
vagrant ssh master

# install docker
curl -fsSL get.docker.com -o get-docker.sh &amp;&amp; \
  chmod u+x $_ &amp;&amp; \
  ./$_ &amp;&amp; \
  sudo usermod -aG docker hadoop

# logout and login again to verify docker installation
exit
vagrant ssh master
whoami # hadoop
docker ps -a

# uncomment PostgreSQL configurations
vim devops/hadoop/file/oozie/config/oozie-site.xml # from host
vim /vagrant/file/oozie/config/oozie-site.xml # from guest

# start postgres on guest machine 
docker run \
  --detach \
  --name oozie-postgres \
  -p 5432:5432 \
  -e POSTGRES_DB=&quot;oozie-db&quot; \
  -e POSTGRES_USER=&quot;postgres&quot; \
  -e POSTGRES_PASSWORD=&quot;password&quot; \
  postgres

# permission issue
# https://github.com/docker-library/postgres/issues/116
# --volume /vol/postgres:/var/lib/postgresql/data

# access container
docker exec -it oozie-postgres bash
psql --username=postgres
# list all databases
\list
\connect oozie-db
# list all tables
\dt
# describe table
\d+ wf_jobs
# list workflow
select * from wf_jobs;
</code></pre>

<p>Install and start Oozie</p>
<pre><code class="bash"># access master node
vagrant ssh master

# login as root
sudo su -

# build, install and init
/vagrant/script/setup_oozie.sh

# start oozie manually (first time only)
su --login hadoop /vagrant/script/bootstrap.sh oozie
</code></pre>

<p><em>It might take a while to build the sources</em></p>
<p>Useful paths</p>
<pre><code class="bash"># data and logs
devops/hadoop/.data/master/oozie # host
/vol/oozie # guest

# (guest) config
/usr/local/oozie/conf

# (hdfs) examples
/user/hadoop/examples
</code></pre>

<h3 id="examples_1">Examples</h3>
<p>Run bundled examples within distribution</p>
<pre><code class="bash"># examples path
.data/master/oozie/examples # host
/vol/oozie/examples # guest

# access master node as hadoop user
vagrant ssh master

export OOZIE_EXAMPLE_PATH=/vol/oozie/examples
export OOZIE_HDFS_PATH=/user/$(whoami)/examples

# open map-reduce job.properties
vim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties

# edit the following properties
nameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml
jobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml
queueName=priority_queue # or default @ fair-scheduler.xml

# upload all the examples
hadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH

# verify uploaded files
hadoop fs -ls -h -R /user/$(whoami)

# run the map-reduce workflow example
oozie job \
  -oozie http://oozie.local:11000/oozie \
  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \
  -run

# verify status
oozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID

# verify result
hadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000

# remove all the examples
hadoop fs -rm -R $OOZIE_HDFS_PATH
</code></pre>

<h3 id="useful-commands">Useful commands</h3>
<ul>
<li>Workflow requires <code>oozie.wf.application.path</code> property</li>
<li>Coordinator requires <code>oozie.coord.application.path</code> property</li>
</ul>
<pre><code class="bash"># verify oozie status
oozie admin \
  -oozie http://oozie.local:11000/oozie \
  -status

# verify workflow or coordinator status
oozie job \
  -oozie http://oozie.local:11000/oozie \
  -info JOB_ID \
  -verbose

# poll workflow or coordinator status
oozie job \
  -oozie http://oozie.local:11000/oozie \
  -poll JOB_ID \
  -interval 10 \
  -timeout 60 \
  -verbose

# find running coordinator
oozie jobs \
  -oozie http://oozie.local:11000/oozie/ \
  -filter status=RUNNING \
  -jobtype coordinator

# suspend|resume|kill coordinator
oozie job \
  -oozie http://oozie.local:11000/oozie/ \
  [-suspend|-resume|-kill] \
  XXX-C

# re-run coordinator's workflow (action)
oozie job \
  -oozie http://oozie.local:11000/oozie/ \
  -rerun XXX-C \
  -action 1,2,3,N

# kill workflow
oozie job \
  -oozie http://oozie.local:11000/oozie/ \
  -kill \
  XXX-W

# re-run all workflow's actions
oozie job \
  -oozie http://oozie.local:11000/oozie/ \
  -rerun \
  XXX-W \
  -Doozie.wf.rerun.failnodes=false
</code></pre>

<p><br></p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../cloud/" class="btn btn-neutral float-right" title="Cloud">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../kafka/" class="btn btn-neutral" title="Kafka"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/niqdev/devops/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../kafka/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../cloud/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '..';</script>
    <script src="../js/theme.js" defer></script>
      <script src="../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
