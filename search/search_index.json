{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DevOps A collection of notes, resources, documentation and POCs mainly related to distributed systems for local development, learning purposes and quick prototyping.","title":"DevOps"},{"location":"#devops","text":"A collection of notes, resources, documentation and POCs mainly related to distributed systems for local development, learning purposes and quick prototyping.","title":"DevOps"},{"location":"ansible/","text":"Ansible Ansible is an open source automation platform that can help with config management, deployment and task automation Resources Documentation Ansible - Up and Running (2017) by Lorin Hochstein and Rene Moser (Book) Tutorial Playbook example Ansible Tutorial for Beginners: Ultimate Playbook & Examples The following guide explains how to provision Ansible locally and play with it. Checkout the Vagrantfile and the Vagrant guide for more details. Setup Requirements Vagrant 2 VirtualBox 5 Directory structure tree -a ansible/ ansible/ \u251c\u2500\u2500 .share \u2502 \u251c\u2500\u2500 node-1 \u2502 \u251c\u2500\u2500 node-2 \u2502 \u251c\u2500\u2500 node-3 \u2502 \u2514\u2500\u2500 ssh \u2502 \u251c\u2500\u2500 ansible_rsa \u2502 \u2514\u2500\u2500 ansible_rsa.pub \u251c\u2500\u2500 Vagrantfile \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 host_vars \u2502 \u251c\u2500\u2500 hosts \u2502 \u251c\u2500\u2500 roles \u2502 \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2502 \u251c\u2500\u2500 defaults \u2502 \u2502 \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u2502 \u251c\u2500\u2500 handlers \u2502 \u2502 \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2502 \u251c\u2500\u2500 tasks \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 motd.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 oracle-jdk.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 package.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 motd \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2502 \u2514\u2500\u2500 docker \u2502 \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2502 \u2514\u2500\u2500 tasks \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2514\u2500\u2500 site.yml \u251c\u2500\u2500 destroy_ansible.sh \u251c\u2500\u2500 setup_ansible.sh \u2514\u2500\u2500 setup_share.sh The first time only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing ./setup_share.sh Start the boxes with vagrant up The first time it could take a while Verify status of the boxes with vagrant status Verify access to the boxes with vagrant ssh ansible vagrant ssh node-1 From inside the boxes you should be able to communicate with the others ping ansible.local ping ip-192-168-100-11.local ping 192.168.100.12 The following paths are shared with the boxes /vagrant provision-tool /local host $HOME /ansible data (ansible only) /data .share (node only) Cleanup ./destroy_ansible.sh Ad-Hoc Commands Access the ansible box with vagrant ssh ansible Below a list of examples # ping all nodes (default inventory /etc/ansible/hosts) ansible all -m ping ansible ansible -m ping ansible cluster -m ping # ping all nodes (specify inventory) ansible all -i \"/vagrant/data/hosts\" -m ping # gathering facts ansible all -m setup ansible ansible -m setup # specify host and user ansible ip-192-168-100-11.local -m ping -u vagrant # execute command ansible all -a \"/bin/echo hello\" ansible all -a \"uptime\" ansible all -a \"/bin/date\" # do NOT reboot vagrant through ansible (use vagrant reload) ansible cluster -a \"/sbin/reboot\" --become # shell module ansible all -m shell -a \"pwd\" # be carefull to quotes ansible all -m shell -a 'echo $HOME' # update && upgrade ansible all -m apt -a \"update_cache=yes upgrade=dist\" --become # restart after upgrade vagrant reload # install package ansible all -m apt -a \"name=tree state=present\" --become Playbooks Access the ansible box with vagrant ssh ansible Below a list of examples # test uptime on all node ansible-playbook /ansible/site.yml --tags=test --verbose # update & upgrade only on cluster nodes ansible-playbook /ansible/site.yml -t package --skip-tags=oracle-jdk --verbose # install oracle-jdk only on cluster nodes ansible-playbook /ansible/site.yml -t oracle-jdk # install all packages on cluster nodes ansible-playbook /ansible/site.yml -t package --verbose # run common task on cluster node ansible-playbook /ansible/site.yml -t common # setup docker ansible-playbook /ansible/site.yml -t docker # test docker vagrant ssh node-1 sudo -i -u docker docker ps -a # custom banner ansible-playbook /ansible/site.yml -t motd # setup all infrastructure at once ansible-playbook /ansible/site.yml # dry run ansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff","title":"Ansible"},{"location":"ansible/#ansible","text":"Ansible is an open source automation platform that can help with config management, deployment and task automation Resources Documentation Ansible - Up and Running (2017) by Lorin Hochstein and Rene Moser (Book) Tutorial Playbook example Ansible Tutorial for Beginners: Ultimate Playbook & Examples The following guide explains how to provision Ansible locally and play with it. Checkout the Vagrantfile and the Vagrant guide for more details.","title":"Ansible"},{"location":"ansible/#setup","text":"Requirements Vagrant 2 VirtualBox 5 Directory structure tree -a ansible/ ansible/ \u251c\u2500\u2500 .share \u2502 \u251c\u2500\u2500 node-1 \u2502 \u251c\u2500\u2500 node-2 \u2502 \u251c\u2500\u2500 node-3 \u2502 \u2514\u2500\u2500 ssh \u2502 \u251c\u2500\u2500 ansible_rsa \u2502 \u2514\u2500\u2500 ansible_rsa.pub \u251c\u2500\u2500 Vagrantfile \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 group_vars \u2502 \u251c\u2500\u2500 host_vars \u2502 \u251c\u2500\u2500 hosts \u2502 \u251c\u2500\u2500 roles \u2502 \u2502 \u251c\u2500\u2500 common \u2502 \u2502 \u2502 \u251c\u2500\u2500 defaults \u2502 \u2502 \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u2502 \u251c\u2500\u2500 handlers \u2502 \u2502 \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2502 \u251c\u2500\u2500 tasks \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 main.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 motd.yml \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 oracle-jdk.yml \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 package.yml \u2502 \u2502 \u2502 \u251c\u2500\u2500 templates \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 motd \u2502 \u2502 \u2502 \u2514\u2500\u2500 vars \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2502 \u2514\u2500\u2500 docker \u2502 \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2502 \u2514\u2500\u2500 tasks \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u2514\u2500\u2500 site.yml \u251c\u2500\u2500 destroy_ansible.sh \u251c\u2500\u2500 setup_ansible.sh \u2514\u2500\u2500 setup_share.sh The first time only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing ./setup_share.sh Start the boxes with vagrant up The first time it could take a while Verify status of the boxes with vagrant status Verify access to the boxes with vagrant ssh ansible vagrant ssh node-1 From inside the boxes you should be able to communicate with the others ping ansible.local ping ip-192-168-100-11.local ping 192.168.100.12 The following paths are shared with the boxes /vagrant provision-tool /local host $HOME /ansible data (ansible only) /data .share (node only) Cleanup ./destroy_ansible.sh","title":"Setup"},{"location":"ansible/#ad-hoc-commands","text":"Access the ansible box with vagrant ssh ansible Below a list of examples # ping all nodes (default inventory /etc/ansible/hosts) ansible all -m ping ansible ansible -m ping ansible cluster -m ping # ping all nodes (specify inventory) ansible all -i \"/vagrant/data/hosts\" -m ping # gathering facts ansible all -m setup ansible ansible -m setup # specify host and user ansible ip-192-168-100-11.local -m ping -u vagrant # execute command ansible all -a \"/bin/echo hello\" ansible all -a \"uptime\" ansible all -a \"/bin/date\" # do NOT reboot vagrant through ansible (use vagrant reload) ansible cluster -a \"/sbin/reboot\" --become # shell module ansible all -m shell -a \"pwd\" # be carefull to quotes ansible all -m shell -a 'echo $HOME' # update && upgrade ansible all -m apt -a \"update_cache=yes upgrade=dist\" --become # restart after upgrade vagrant reload # install package ansible all -m apt -a \"name=tree state=present\" --become","title":"Ad-Hoc Commands"},{"location":"ansible/#playbooks","text":"Access the ansible box with vagrant ssh ansible Below a list of examples # test uptime on all node ansible-playbook /ansible/site.yml --tags=test --verbose # update & upgrade only on cluster nodes ansible-playbook /ansible/site.yml -t package --skip-tags=oracle-jdk --verbose # install oracle-jdk only on cluster nodes ansible-playbook /ansible/site.yml -t oracle-jdk # install all packages on cluster nodes ansible-playbook /ansible/site.yml -t package --verbose # run common task on cluster node ansible-playbook /ansible/site.yml -t common # setup docker ansible-playbook /ansible/site.yml -t docker # test docker vagrant ssh node-1 sudo -i -u docker docker ps -a # custom banner ansible-playbook /ansible/site.yml -t motd # setup all infrastructure at once ansible-playbook /ansible/site.yml # dry run ansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff","title":"Playbooks"},{"location":"azure/","text":"Azure ARM template documentation Azure Automation documentation Azure Security Center documentation","title":"Azure"},{"location":"azure/#azure","text":"ARM template documentation Azure Automation documentation Azure Security Center documentation","title":"Azure"},{"location":"cassandra/","text":"Cassandra Cassandra is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure Resources Documentation Cassandra: The Definitive Guide (2016)(4th) by Eben Hewitt, Jeff Carpenter (Book) A Decentralized Structured Storage System (Paper) A Big Data Modeling Methodology for Apache Cassandra (Paper) Facebook\u2019s Cassandra paper Cassandra Data Modeling Best Practices Difference between partition key, composite key and clustering key Cassandra Cluster Manager Netflix Priam cstar_perf Amy's Cassandra 2.1 tuning guide Repair in Cassandra Cassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes Architecture A rack is a logical set of nodes in close proximity to each other A data center is a logical set of racks Cassandra uses a gossip protocol (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called Phi Accrual Failure Detection instead of simple heartbeats The job of a snitch is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from Cassandra represents the data managed by a cluster as a ring . Each node in the ring is assigned one or more ranges of data described by a token , which determines its position in the ring and is used to identify each partition virtual nodes allow to break a token range and assign multiple tokens to a single physical node A partitioner is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring The replication factor is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace Cassandra provides tuneable consistency levels and must be specified on each read or write A client may connect to any node in the cluster, named coordinator node , to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them When a write operation is performed, it's immediately written to a commit log to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it After it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the memtable divided by Column Family (table) When the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called SSTable and a new memtable or commit log is then created/recycled No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic compaction operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created The key cache stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap The row cache caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory The counter cache is used to improve counter performance by reducing lock contention for the most frequently accessed counters In a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called hinted handoff . The coordinator node while store temporarily the data until it detects that the node is available again Write Path Read Path To provide linearizable consistency e.g. read-before-write, Cassandra supports a lightweight transaction or LWT. The implementation is based on paxos and is limited to a single partition A tombstone is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation Bloom filters are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure Replica Synchronization (1) Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a read repair is performed immediately to update the out of date replicas Replica Synchronization (2) Anti-entropy repair is a manually initiated operation performed on nodes as part of a regular maintenance process executed with nodetool causing a major compaction during which a node exchange Merkle trees with neighboring nodes Setup Single Node Cluster # change path cd devops/cassandra # start single node docker-compose up # paths /etc/cassandra /var/lib/cassandra /var/log/cassandra # remove container and volume docker rm -fv devops-cassandra Multi Node Cluster # change path cd devops/cassandra # start node docker-compose -f docker-compose-cluster.yml up # optional mounted volumes mkdir -p \\ .cassandra/cassandra-seed/{data,log} \\ .cassandra/cassandra-node-1/{data,log} \\ .cassandra/cassandra-node-2/{data,log} tree .cassandra/ # ISSUES releated to host permissions # > Small commitlog volume detected at /var/lib/cassandra/commitlog # > There is insufficient memory for the Java Runtime Environment to continue (cassandra) /var/lib/cassandra (root) /var/log/cassandra Access container # access container docker exec -it devops-cassandra bash docker exec -it devops-cassandra bash -c cqlsh docker exec -it devops-cassandra-seed bash docker exec -it devops-cassandra-node-1 bash # execute cql script from host (docker exec -i devops-cassandra bash \\ -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql CQL cqlsh script examples # connect cqlsh localhost 9042 cqlsh localhost 9042 -u cassandra -p cassandra # execute cql script cqlsh -f cql/example_create.cql # info SHOW VERSION; DESCRIBE CLUSTER; DESCRIBE KEYSPACES; DESCRIBE KEYSPACE example; DESCRIBE TABLE example.messages; # nice format EXPAND ON; # trace query TRACING ON; # bulk loading COPY example.users TO '/cql/users.csv' WITH HEADER=TRUE; COPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';'; COPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE; # automatic paging PAGING; PAGING ON; PAGING 100; # limit SELECT * FROM example.users LIMIT 1; Batch User-Defined Type User-Defined Function User-Defined Aggregate Function Old cassandra-cli deprecated and removed in Cassandra 3.0 USE keyspace_name; LIST table_name; GET table_name[\"primary_key\"]; SET table_name[\"primary_key\"][\"column_name\"]; nodetool # help nodetool # cluster informations nodetool describecluster nodetool status # node informations nodetool -h xxx.xxx.xxx.xxx info nodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff nodetool gossipinfo # ring informations nodetool ring nodetool describering KEYSPACE # monitor network nodetool netstats # threadpool statistics nodetool tpstats # keyspace statistics nodetool tablestats KEYSPACE # dynamic logging via JMX nodetool getlogginglevels # force to write data from memtables to SSTables nodetool flush # gracefully shutdown nodetool drain # discards any data that is no longer owned by the node # e.g. after changing replication factor or token range nodetool cleanup # anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes # maintenance: incremental parallel repair on the primary token range (run on each node) nodetool repair -pr # create snapshot nodetool snapshot nodetool listsnapshots # restore snapshot (create schema or truncate table before) # 1) same cluster and configuration # copy SSTable \".db\" files into the data directory and on the running node execute refresh nodetool refresh # 2) different configuration (e.g. topology, token ranges, or replication) sstableloader # stress tool cassandra-stress write n=1000000 cassandra-stress read n=200000","title":"Cassandra"},{"location":"cassandra/#cassandra","text":"Cassandra is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure Resources Documentation Cassandra: The Definitive Guide (2016)(4th) by Eben Hewitt, Jeff Carpenter (Book) A Decentralized Structured Storage System (Paper) A Big Data Modeling Methodology for Apache Cassandra (Paper) Facebook\u2019s Cassandra paper Cassandra Data Modeling Best Practices Difference between partition key, composite key and clustering key Cassandra Cluster Manager Netflix Priam cstar_perf Amy's Cassandra 2.1 tuning guide Repair in Cassandra Cassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes","title":"Cassandra"},{"location":"cassandra/#architecture","text":"A rack is a logical set of nodes in close proximity to each other A data center is a logical set of racks Cassandra uses a gossip protocol (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called Phi Accrual Failure Detection instead of simple heartbeats The job of a snitch is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from Cassandra represents the data managed by a cluster as a ring . Each node in the ring is assigned one or more ranges of data described by a token , which determines its position in the ring and is used to identify each partition virtual nodes allow to break a token range and assign multiple tokens to a single physical node A partitioner is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring The replication factor is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace Cassandra provides tuneable consistency levels and must be specified on each read or write A client may connect to any node in the cluster, named coordinator node , to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them When a write operation is performed, it's immediately written to a commit log to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it After it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the memtable divided by Column Family (table) When the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called SSTable and a new memtable or commit log is then created/recycled No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic compaction operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created The key cache stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap The row cache caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory The counter cache is used to improve counter performance by reducing lock contention for the most frequently accessed counters In a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called hinted handoff . The coordinator node while store temporarily the data until it detects that the node is available again Write Path Read Path To provide linearizable consistency e.g. read-before-write, Cassandra supports a lightweight transaction or LWT. The implementation is based on paxos and is limited to a single partition A tombstone is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation Bloom filters are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure Replica Synchronization (1) Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a read repair is performed immediately to update the out of date replicas Replica Synchronization (2) Anti-entropy repair is a manually initiated operation performed on nodes as part of a regular maintenance process executed with nodetool causing a major compaction during which a node exchange Merkle trees with neighboring nodes","title":"Architecture"},{"location":"cassandra/#setup","text":"Single Node Cluster # change path cd devops/cassandra # start single node docker-compose up # paths /etc/cassandra /var/lib/cassandra /var/log/cassandra # remove container and volume docker rm -fv devops-cassandra Multi Node Cluster # change path cd devops/cassandra # start node docker-compose -f docker-compose-cluster.yml up # optional mounted volumes mkdir -p \\ .cassandra/cassandra-seed/{data,log} \\ .cassandra/cassandra-node-1/{data,log} \\ .cassandra/cassandra-node-2/{data,log} tree .cassandra/ # ISSUES releated to host permissions # > Small commitlog volume detected at /var/lib/cassandra/commitlog # > There is insufficient memory for the Java Runtime Environment to continue (cassandra) /var/lib/cassandra (root) /var/log/cassandra Access container # access container docker exec -it devops-cassandra bash docker exec -it devops-cassandra bash -c cqlsh docker exec -it devops-cassandra-seed bash docker exec -it devops-cassandra-node-1 bash # execute cql script from host (docker exec -i devops-cassandra bash \\ -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql","title":"Setup"},{"location":"cassandra/#cql","text":"cqlsh script examples # connect cqlsh localhost 9042 cqlsh localhost 9042 -u cassandra -p cassandra # execute cql script cqlsh -f cql/example_create.cql # info SHOW VERSION; DESCRIBE CLUSTER; DESCRIBE KEYSPACES; DESCRIBE KEYSPACE example; DESCRIBE TABLE example.messages; # nice format EXPAND ON; # trace query TRACING ON; # bulk loading COPY example.users TO '/cql/users.csv' WITH HEADER=TRUE; COPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';'; COPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE; # automatic paging PAGING; PAGING ON; PAGING 100; # limit SELECT * FROM example.users LIMIT 1; Batch User-Defined Type User-Defined Function User-Defined Aggregate Function Old cassandra-cli deprecated and removed in Cassandra 3.0 USE keyspace_name; LIST table_name; GET table_name[\"primary_key\"]; SET table_name[\"primary_key\"][\"column_name\"];","title":"CQL"},{"location":"cassandra/#nodetool","text":"# help nodetool # cluster informations nodetool describecluster nodetool status # node informations nodetool -h xxx.xxx.xxx.xxx info nodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff nodetool gossipinfo # ring informations nodetool ring nodetool describering KEYSPACE # monitor network nodetool netstats # threadpool statistics nodetool tpstats # keyspace statistics nodetool tablestats KEYSPACE # dynamic logging via JMX nodetool getlogginglevels # force to write data from memtables to SSTables nodetool flush # gracefully shutdown nodetool drain # discards any data that is no longer owned by the node # e.g. after changing replication factor or token range nodetool cleanup # anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes # maintenance: incremental parallel repair on the primary token range (run on each node) nodetool repair -pr # create snapshot nodetool snapshot nodetool listsnapshots # restore snapshot (create schema or truncate table before) # 1) same cluster and configuration # copy SSTable \".db\" files into the data directory and on the running node execute refresh nodetool refresh # 2) different configuration (e.g. topology, token ranges, or replication) sstableloader # stress tool cassandra-stress write n=1000000 cassandra-stress read n=200000","title":"nodetool"},{"location":"cloud/","text":"Cloud CNCF cloud native landscape CloudSkew - Draw cloud architecture diagrams Steampipe - select * from cloud; Infracost - Cloud cost estimates for Terraform in pull requests Rover - Terraform Visualizer AWS AWS diagrams & notes cfn-diagram - Visualise CloudFormation/SAM/CDK templates as diagrams CDK-Dia - Automated diagrams for CDK infrastructure Kubernetes Resources Kube by Example Kubernetes The Hard Way Kubernetes Best Practices 101 Kubernetes Failure Stories 10 most common mistakes using kubernetes Container Training 15 Kubernetes Best Practices Every Developer Should Know Tools Kubetools - A Curated List of Kubernetes Tools KEDA - Kubernetes Event-driven Autoscaling Mizu - API Traffic Viewer for Kubernetes Sloop - Kubernetes History Visualization Atlas - Effortless deployment pipelines for Kubernetes kube-chaos minikube K3s - Lightweight Kubernetes devtron - Tool integration platform for Kubernetes arkade - Open Source Marketplace For Kubernetes Kubernetes YAML Generator kind - Local Kubernetes clusters using Docker Cli Kustomize - Customization of Kubernetes YAML configurations Krew - Find and install kubectl plugins kubectx - A tool to switch between Kubernetes contexts Display the current kubectl context in the Bash prompt Kubie - A more powerful alternative to kubectx and kubens kube-ps1 - Kubernetes prompt kube-prompt - An interactive kubernetes client featuring auto-complete kubeprompt - Isolated kubectl shells and prompt info kubefwd - Kubernetes port forwarding for local development stern - Multi pod and container log tailing for Kubernetes (obsolete) kail - kubernetes log viewer k9s - Kubernetes CLI To Manage Your Clusters In Style! KDash - A fast and simple dashboard for Kubernetes Skaffold - Local Kubernetes Development Homelab k8s@home k3sup Dan Manners' Homelab Khue's Homelab Humble Project Truxnell's home k8s cluster Raspberry Pi Setup Kubernetes on a Raspberry Pi Cluster easily the official way! Raspberry Pi Kubernetes Cluster Building an ARM Kubernetes Cluster kube-arm Other Kubelist Podcast Kubernetes comic","title":"Cloud"},{"location":"cloud/#cloud","text":"CNCF cloud native landscape CloudSkew - Draw cloud architecture diagrams Steampipe - select * from cloud; Infracost - Cloud cost estimates for Terraform in pull requests Rover - Terraform Visualizer","title":"Cloud"},{"location":"cloud/#aws","text":"AWS diagrams & notes cfn-diagram - Visualise CloudFormation/SAM/CDK templates as diagrams CDK-Dia - Automated diagrams for CDK infrastructure","title":"AWS"},{"location":"cloud/#kubernetes","text":"","title":"Kubernetes"},{"location":"cloud/#resources","text":"Kube by Example Kubernetes The Hard Way Kubernetes Best Practices 101 Kubernetes Failure Stories 10 most common mistakes using kubernetes Container Training 15 Kubernetes Best Practices Every Developer Should Know","title":"Resources"},{"location":"cloud/#tools","text":"Kubetools - A Curated List of Kubernetes Tools KEDA - Kubernetes Event-driven Autoscaling Mizu - API Traffic Viewer for Kubernetes Sloop - Kubernetes History Visualization Atlas - Effortless deployment pipelines for Kubernetes kube-chaos minikube K3s - Lightweight Kubernetes devtron - Tool integration platform for Kubernetes arkade - Open Source Marketplace For Kubernetes Kubernetes YAML Generator kind - Local Kubernetes clusters using Docker","title":"Tools"},{"location":"cloud/#cli","text":"Kustomize - Customization of Kubernetes YAML configurations Krew - Find and install kubectl plugins kubectx - A tool to switch between Kubernetes contexts Display the current kubectl context in the Bash prompt Kubie - A more powerful alternative to kubectx and kubens kube-ps1 - Kubernetes prompt kube-prompt - An interactive kubernetes client featuring auto-complete kubeprompt - Isolated kubectl shells and prompt info kubefwd - Kubernetes port forwarding for local development stern - Multi pod and container log tailing for Kubernetes (obsolete) kail - kubernetes log viewer k9s - Kubernetes CLI To Manage Your Clusters In Style! KDash - A fast and simple dashboard for Kubernetes Skaffold - Local Kubernetes Development","title":"Cli"},{"location":"cloud/#homelab","text":"k8s@home k3sup Dan Manners' Homelab Khue's Homelab Humble Project Truxnell's home k8s cluster","title":"Homelab"},{"location":"cloud/#raspberry-pi","text":"Setup Kubernetes on a Raspberry Pi Cluster easily the official way! Raspberry Pi Kubernetes Cluster Building an ARM Kubernetes Cluster kube-arm","title":"Raspberry Pi"},{"location":"cloud/#other","text":"Kubelist Podcast Kubernetes comic","title":"Other"},{"location":"docker/","text":"Docker Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications Resources Documentation Docker in Action (2016) by Jeff Nickoloff (Book) How-To Setup # install docker curl -fsSL get.docker.com -o get-docker.sh && \\ chmod u+x $_ && \\ ./$_ && \\ sudo usermod -aG docker docker docker --version # install docker-compose sudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\ -o /usr/local/bin/docker-compose && \\ sudo chmod +x /usr/local/bin/docker-compose docker-compose --version # install docker-machine (VirtualBox required) curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\ sudo install /tmp/docker-machine /usr/local/bin/docker-machine docker-machine --version Useful commands # list images docker images # list containers docker ps -a # list volumes docker volume ls # run temporary container docker run --rm --name phusion phusion/baseimage:latest # access container from another shell docker exec -it phusion bash # remove container by name docker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f # delete dangling images <none> docker images -q -f dangling=true | xargs --no-run-if-empty docker rmi # delete dangling volumes docker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm Docker Machine # create local machine docker-machine create --driver virtualbox default # list docker-machine ls docker-machine ls --filter name=default docker-machine ls --filter state=Running docker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\" # info docker-machine inspect default docker-machine inspect --format='{{.Driver.IPAddress}}' default docker-machine status default docker-machine ip default # management docker-machine start default docker-machine stop default docker-machine restart default docker-machine rm default # mount volume #https://docs.docker.com/machine/reference/mount # show command to connect to machine docker-machine env default # check if variables are set env | grep DOCKER # connect to machine eval \"$(docker-machine env default)\" docker ps -a # show command to disconnect from machine docker-machine env -u # unset all eval $(docker-machine env -u) # access docker-machine ssh default # execute command and exit docker-machine ssh default uptime # copy files from host to guest docker-machine scp -r /FROM default:/TO # start nginx on default machine docker run -d -p 8000:80 nginx # verify from host curl $(docker-machine ip default):8000 # forward to port 8080 docker-machine ssh default -L 8080:localhost:8000 # verify tunnel from host curl localhost:8080 # disable error crash reporting mkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report Base image Supervisor Build devops/base image # change path cd devops/base # build image docker build -t devops/base . # temporary container docker run --rm --name devops-base devops/base # access container docker exec -it devops-base bash # configurations /etc/supervisor/conf.d # supervisor actions supervisorctl status supervisorctl start SERVICE_NAME supervisorctl stop SERVICE_NAME Docker Hub niqdev/phusion-base niqdev/zookeeper niqdev/kafka docker login # phusion-base # https://github.com/phusion/baseimage-docker docker build -t devops/base:latest ./base docker tag devops/base niqdev/phusion-base:latest-amd64 docker tag devops/base niqdev/phusion-base:latest docker push niqdev/phusion-base:latest-amd64 docker push niqdev/phusion-base:latest # zookeeper docker build -t devops/zookeeper:latest ./zookeeper docker tag devops/zookeeper niqdev/zookeeper:3.5.5 docker tag devops/zookeeper niqdev/zookeeper docker push niqdev/zookeeper:3.5.5 docker push niqdev/zookeeper:latest # kafka docker build -t devops/kafka:latest ./kafka docker tag devops/kafka niqdev/kafka:2.3.0 docker tag devops/kafka niqdev/kafka docker push niqdev/kafka:2.3.0 docker push niqdev/kafka:latest docker-compose -f kafka/docker-compose-hub.yml up","title":"Docker"},{"location":"docker/#docker","text":"Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications Resources Documentation Docker in Action (2016) by Jeff Nickoloff (Book)","title":"Docker"},{"location":"docker/#how-to","text":"Setup # install docker curl -fsSL get.docker.com -o get-docker.sh && \\ chmod u+x $_ && \\ ./$_ && \\ sudo usermod -aG docker docker docker --version # install docker-compose sudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\ -o /usr/local/bin/docker-compose && \\ sudo chmod +x /usr/local/bin/docker-compose docker-compose --version # install docker-machine (VirtualBox required) curl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\ sudo install /tmp/docker-machine /usr/local/bin/docker-machine docker-machine --version Useful commands # list images docker images # list containers docker ps -a # list volumes docker volume ls # run temporary container docker run --rm --name phusion phusion/baseimage:latest # access container from another shell docker exec -it phusion bash # remove container by name docker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f # delete dangling images <none> docker images -q -f dangling=true | xargs --no-run-if-empty docker rmi # delete dangling volumes docker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm Docker Machine # create local machine docker-machine create --driver virtualbox default # list docker-machine ls docker-machine ls --filter name=default docker-machine ls --filter state=Running docker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\" # info docker-machine inspect default docker-machine inspect --format='{{.Driver.IPAddress}}' default docker-machine status default docker-machine ip default # management docker-machine start default docker-machine stop default docker-machine restart default docker-machine rm default # mount volume #https://docs.docker.com/machine/reference/mount # show command to connect to machine docker-machine env default # check if variables are set env | grep DOCKER # connect to machine eval \"$(docker-machine env default)\" docker ps -a # show command to disconnect from machine docker-machine env -u # unset all eval $(docker-machine env -u) # access docker-machine ssh default # execute command and exit docker-machine ssh default uptime # copy files from host to guest docker-machine scp -r /FROM default:/TO # start nginx on default machine docker run -d -p 8000:80 nginx # verify from host curl $(docker-machine ip default):8000 # forward to port 8080 docker-machine ssh default -L 8080:localhost:8000 # verify tunnel from host curl localhost:8080 # disable error crash reporting mkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report","title":"How-To"},{"location":"docker/#base-image","text":"Supervisor Build devops/base image # change path cd devops/base # build image docker build -t devops/base . # temporary container docker run --rm --name devops-base devops/base # access container docker exec -it devops-base bash # configurations /etc/supervisor/conf.d # supervisor actions supervisorctl status supervisorctl start SERVICE_NAME supervisorctl stop SERVICE_NAME","title":"Base image"},{"location":"docker/#docker-hub","text":"niqdev/phusion-base niqdev/zookeeper niqdev/kafka docker login # phusion-base # https://github.com/phusion/baseimage-docker docker build -t devops/base:latest ./base docker tag devops/base niqdev/phusion-base:latest-amd64 docker tag devops/base niqdev/phusion-base:latest docker push niqdev/phusion-base:latest-amd64 docker push niqdev/phusion-base:latest # zookeeper docker build -t devops/zookeeper:latest ./zookeeper docker tag devops/zookeeper niqdev/zookeeper:3.5.5 docker tag devops/zookeeper niqdev/zookeeper docker push niqdev/zookeeper:3.5.5 docker push niqdev/zookeeper:latest # kafka docker build -t devops/kafka:latest ./kafka docker tag devops/kafka niqdev/kafka:2.3.0 docker tag devops/kafka niqdev/kafka docker push niqdev/kafka:2.3.0 docker push niqdev/kafka:latest docker-compose -f kafka/docker-compose-hub.yml up","title":"Docker Hub"},{"location":"hadoop/","text":"Hadoop The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the Vagrantfile and the Vagrant guide for more details. Resources Documentation Hadoop: The Definitive Guide (2015)(4th) by Tom White (Book) The Hadoop Ecosystem Table Hadoop Internals Setup Requirements Vagrant VirtualBox Directory structure tree -a hadoop/ hadoop/ \u251c\u2500\u2500 .data # mounted volume \u2502 \u251c\u2500\u2500 hadoop_rsa \u2502 \u251c\u2500\u2500 hadoop_rsa.pub \u2502 \u251c\u2500\u2500 master \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mapred \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 yarn \u2502 \u2502 \u2502 \u251c\u2500\u2500 namenode \u2502 \u2502 \u2502 \u2514\u2500\u2500 secondary \u2502 \u2502 \u251c\u2500\u2500 oozie \u2502 \u2502 \u2502 \u251c\u2500\u2500 data \u2502 \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u251c\u2500\u2500 spark \u2502 \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u2514\u2500\u2500 zeppelin \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2514\u2500\u2500 notebook \u2502 \u251c\u2500\u2500 node-1 \u2502 \u2502 \u2514\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 datanode \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 mapred \u2502 \u2502 \u2514\u2500\u2500 yarn \u2502 \u251c\u2500\u2500 node-2 \u2502 \u251c\u2500\u2500 node-3 \u251c\u2500\u2500 example \u2502 \u251c\u2500\u2500 map-reduce \u2502 \u2514\u2500\u2500 spark \u251c\u2500\u2500 file \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 core-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 fair-scheduler.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 hdfs-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 mapred-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 masters \u2502 \u2502 \u2502 \u251c\u2500\u2500 slaves \u2502 \u2502 \u2502 \u2514\u2500\u2500 yarn-site.xml \u2502 \u2502 \u2514\u2500\u2500 profile-hadoop.sh \u2502 \u251c\u2500\u2500 hosts \u2502 \u251c\u2500\u2500 motd \u2502 \u251c\u2500\u2500 oozie \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 oozie-env.sh \u2502 \u2502 \u2502 \u2514\u2500\u2500 oozie-site.xml \u2502 \u2502 \u2514\u2500\u2500 profile-oozie.sh \u2502 \u251c\u2500\u2500 spark \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 log4j.properties \u2502 \u2502 \u2502 \u2514\u2500\u2500 spark-env.sh \u2502 \u2502 \u2514\u2500\u2500 profile-spark.sh \u2502 \u251c\u2500\u2500 ssh \u2502 \u2502 \u2514\u2500\u2500 config \u2502 \u2514\u2500\u2500 zeppelin \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2514\u2500\u2500 zeppelin-env.sh \u2502 \u2514\u2500\u2500 profile-zeppelin.sh \u251c\u2500\u2500 script \u2502 \u251c\u2500\u2500 bootstrap.sh \u2502 \u251c\u2500\u2500 setup_hadoop.sh \u2502 \u251c\u2500\u2500 setup_oozie.sh \u2502 \u251c\u2500\u2500 setup_spark.sh \u2502 \u251c\u2500\u2500 setup_ubuntu.sh \u2502 \u2514\u2500\u2500 setup_zeppelin.sh \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 vagrant_hadoop.sh Import the script source vagrant_hadoop.sh Create and start a Multi Node Hadoop Cluster hadoop-start The first time it might take a while Access the cluster via ssh, check also the /etc/hosts file vagrant ssh master ssh hadoop@172.16.0.10 -i .data/hadoop_rsa # 3 nodes vagrant ssh node-1 ssh hadoop@172.16.0.101 -i .data/hadoop_rsa Destroy the cluster hadoop-destroy For convenience add to the host machine cat hadoop/file/hosts | sudo tee --append /etc/hosts Web UI links NameNode: http://namenode.local:50070 NameNode metrics: http://namenode.local:50070/jmx ResourceManager: http://resource-manager.local:8088 Log Level: http://resource-manager.local:8088/logLevel Web Application Proxy Server: http://web-proxy.local:8100/proxy/application_XXX_0000 MapReduce Job History Server: http://history.local:19888 DataNode/NodeManager (1): http://node-1.local:8042/node DataNode/NodeManager (2): http://node-2.local:8042/node DataNode/NodeManager (3): http://node-3.local:8042/node Spark: http://spark.local:4040 Spark History Server: http://spark-history.local:18080 Zeppelin (*): http://zeppelin.local:8080 Oozie (*): http://oozie.local:11000 (*) Not installed by default HDFS and MapReduce HDFS is a distributed file system that provides high-throughput access to application data YARN is a framework for job scheduling and cluster resource management MapReduce is a YARN-based system for parallel processing of large data sets Documentation Hadoop v2.7.6 Untangling Apache Hadoop YARN series Admin HDFS cli # help hdfs # filesystem statistics hdfs dfsadmin -report # filesystem check hdfs fsck / YARN cli # help yarn # list yarn applications yarn application -list # list nodes yarn node -list # view application logs yarn logs -applicationId APPLICATION_ID # kill yarn application yarn application -kill APPLICATION_ID Useful paths # data and logs devops/hadoop/.data/master/hadoop # host /vol/hadoop # guest # (guest) config /usr/local/hadoop/etc/hadoop # (hdfs) map-reduce history /mr-history/history/done_intermediate/hadoop # (hdfs) aggregated app logs /yarn/app/hadoop/logs/application_XXX MapReduce WordCount Job # build jar on the host machine cd devops/hadoop/example/map-reduce ./gradlew clean build cd devops/hadoop vagrant ssh master # create base directory using hdfs hdfs dfs -mkdir -p /user/ubuntu # create example directory hadoop fs -mkdir -p /user/ubuntu/word-count/input # list directory hadoop fs -ls -h -R / hadoop fs -ls -h -R /user/ubuntu # create sample files echo \"Hello World Bye World\" > file01 echo \"Hello Hadoop Goodbye Hadoop\" > file02 # copy from local to hdfs hadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input hadoop fs -put file02 /user/ubuntu/word-count/input # verify copied files hadoop fs -ls -h -R /user/ubuntu hadoop fs -cat /user/ubuntu/word-count/input/file01 hadoop fs -cat /user/ubuntu/word-count/input/file02 hadoop fs -cat /user/ubuntu/word-count/input/* # run application hadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\ /user/ubuntu/word-count/input \\ /user/ubuntu/word-count/output # check output hadoop fs -cat /user/ubuntu/word-count/output/part-r-00000 # delete directory to run it again hadoop fs -rm -R /user/ubuntu/word-count/output # run sample job in a different queue hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ wordcount \\ -Dmapreduce.job.queuename=root.priority_queue \\ /user/ubuntu/word-count/input \\ /user/ubuntu/word-count/output # well known WARN issue # https://issues.apache.org/jira/browse/HDFS-10429 Benchmarking MapReduce with TeraSort # generate random data hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ teragen 1000 random-data # run terasort benchmark hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ terasort random-data sorted-data # validate data hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ teravalidate sorted-data report # useful commands hadoop fs -ls -h -R . hadoop fs -rm -r random-data hadoop fs -cat random-data/part-m-00000 hadoop fs -cat sorted-data/part-r-00000 Spark Spark is an open-source cluster-computing framework Resources Documentation Spark in Action (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i (Book) Big Data Analysis with Scala and Spark (Course) How-to: Tune Your Apache Spark Jobs series Understanding Resource Allocation configurations for a Spark application Apache Spark: Config Cheatsheet Mastering Apache Spark Managing Spark Partitions with Coalesce and Repartition Understanding Apache Spark on YARN Spark application on YARN # start REPL spark-shell pyspark Interactive Analysis example spark-shell # spark shell with yarn spark-shell --master yarn --deploy-mode client # view all configured parameters sc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\")) val licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\") val lineCount = licenceLines.count val isBsd = (line: String) => line.contains(\"BSD\") val bsdLines = licenceLines.filter(isBsd) bsdLines.count bsdLines.foreach(println) Spark Job examples Example local # run SparkPi example spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master local[*] \\ $SPARK_HOME/examples/jars/spark-examples_*.jar 10 # GitHub event documentation # https://developer.github.com/v3/activity/events/types # build jar on the host machine cd devops/hadoop/example/spark sbt clean package cd devops/hadoop vagrant ssh master # sample dataset mkdir -p github-archive && \\ cd $_ && \\ wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\ gunzip -k * # sample line head -n 1 2018-01-01-0.json | jq '.' # run local job spark-submit \\ --class \"com.github.niqdev.App\" \\ --master local[*] \\ /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar Example cluster # run job in YARN cluster-deploy mode spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 2g \\ --executor-memory 1g \\ --executor-cores 3 \\ --queue default \\ $SPARK_HOME/examples/jars/spark-examples*.jar \\ 10 # --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\" Zeppelin Zeppelin is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more Resources Documentation Setup Install and start Zeppelin # access master node vagrant ssh master # login as root sudo su - # install and init /vagrant/script/setup_zeppelin.sh # start manually (first time only) su --login hadoop /vagrant/script/bootstrap.sh zeppelin Examples Learning Spark SQL with Zeppelin # markdown interpreter %md hello # shell interpreter %sh hadoop fs -ls -h -R / Cluster issue: verify to have enough memory with free -m e.g. Error: Cannot allocate memory Oozie Oozie is a workflow scheduler system to manage Hadoop jobs Resources Documentation Setup Optional PostgreSQL configuration - By default Oozie is configured to use Embedded Derby # access master node vagrant ssh master # install docker curl -fsSL get.docker.com -o get-docker.sh && \\ chmod u+x $_ && \\ ./$_ && \\ sudo usermod -aG docker hadoop # logout and login again to verify docker installation exit vagrant ssh master whoami # hadoop docker ps -a # uncomment PostgreSQL configurations vim devops/hadoop/file/oozie/config/oozie-site.xml # from host vim /vagrant/file/oozie/config/oozie-site.xml # from guest # start postgres on guest machine docker run \\ --detach \\ --name oozie-postgres \\ -p 5432:5432 \\ -e POSTGRES_DB=\"oozie-db\" \\ -e POSTGRES_USER=\"postgres\" \\ -e POSTGRES_PASSWORD=\"password\" \\ postgres # permission issue # https://github.com/docker-library/postgres/issues/116 # --volume /vol/postgres:/var/lib/postgresql/data # access container docker exec -it oozie-postgres bash psql --username=postgres # list all databases \\list \\connect oozie-db # list all tables \\dt # describe table \\d+ wf_jobs # list workflow select * from wf_jobs; Install and start Oozie # access master node vagrant ssh master # login as root sudo su - # build, install and init /vagrant/script/setup_oozie.sh # start oozie manually (first time only) su --login hadoop /vagrant/script/bootstrap.sh oozie It might take a while to build the sources Useful paths # data and logs devops/hadoop/.data/master/oozie # host /vol/oozie # guest # (guest) config /usr/local/oozie/conf # (hdfs) examples /user/hadoop/examples Examples Run bundled examples within distribution # examples path .data/master/oozie/examples # host /vol/oozie/examples # guest # access master node as hadoop user vagrant ssh master export OOZIE_EXAMPLE_PATH=/vol/oozie/examples export OOZIE_HDFS_PATH=/user/$(whoami)/examples # open map-reduce job.properties vim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties # edit the following properties nameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml jobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml queueName=priority_queue # or default @ fair-scheduler.xml # upload all the examples hadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH # verify uploaded files hadoop fs -ls -h -R /user/$(whoami) # run the map-reduce workflow example oozie job \\ -oozie http://oozie.local:11000/oozie \\ -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\ -run # verify status oozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID # verify result hadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000 # remove all the examples hadoop fs -rm -R $OOZIE_HDFS_PATH Useful commands Workflow requires oozie.wf.application.path property Coordinator requires oozie.coord.application.path property # verify oozie status oozie admin \\ -oozie http://oozie.local:11000/oozie \\ -status # verify workflow or coordinator status oozie job \\ -oozie http://oozie.local:11000/oozie \\ -info JOB_ID \\ -verbose # poll workflow or coordinator status oozie job \\ -oozie http://oozie.local:11000/oozie \\ -poll JOB_ID \\ -interval 10 \\ -timeout 60 \\ -verbose # find running coordinator oozie jobs \\ -oozie http://oozie.local:11000/oozie/ \\ -filter status=RUNNING \\ -jobtype coordinator # suspend|resume|kill coordinator oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ [-suspend|-resume|-kill] \\ XXX-C # re-run coordinator's workflow (action) oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -rerun XXX-C \\ -action 1,2,3,N # kill workflow oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -kill \\ XXX-W # re-run all workflow's actions oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -rerun \\ XXX-W \\ -Doozie.wf.rerun.failnodes=false","title":"Hadoop"},{"location":"hadoop/#hadoop","text":"The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the Vagrantfile and the Vagrant guide for more details. Resources Documentation Hadoop: The Definitive Guide (2015)(4th) by Tom White (Book) The Hadoop Ecosystem Table Hadoop Internals","title":"Hadoop"},{"location":"hadoop/#setup","text":"Requirements Vagrant VirtualBox Directory structure tree -a hadoop/ hadoop/ \u251c\u2500\u2500 .data # mounted volume \u2502 \u251c\u2500\u2500 hadoop_rsa \u2502 \u251c\u2500\u2500 hadoop_rsa.pub \u2502 \u251c\u2500\u2500 master \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 mapred \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 yarn \u2502 \u2502 \u2502 \u251c\u2500\u2500 namenode \u2502 \u2502 \u2502 \u2514\u2500\u2500 secondary \u2502 \u2502 \u251c\u2500\u2500 oozie \u2502 \u2502 \u2502 \u251c\u2500\u2500 data \u2502 \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u251c\u2500\u2500 spark \u2502 \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u2514\u2500\u2500 zeppelin \u2502 \u2502 \u251c\u2500\u2500 log \u2502 \u2502 \u2514\u2500\u2500 notebook \u2502 \u251c\u2500\u2500 node-1 \u2502 \u2502 \u2514\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 datanode \u2502 \u2502 \u2514\u2500\u2500 log \u2502 \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 mapred \u2502 \u2502 \u2514\u2500\u2500 yarn \u2502 \u251c\u2500\u2500 node-2 \u2502 \u251c\u2500\u2500 node-3 \u251c\u2500\u2500 example \u2502 \u251c\u2500\u2500 map-reduce \u2502 \u2514\u2500\u2500 spark \u251c\u2500\u2500 file \u2502 \u251c\u2500\u2500 hadoop \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 core-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 fair-scheduler.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 hdfs-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 mapred-site.xml \u2502 \u2502 \u2502 \u251c\u2500\u2500 masters \u2502 \u2502 \u2502 \u251c\u2500\u2500 slaves \u2502 \u2502 \u2502 \u2514\u2500\u2500 yarn-site.xml \u2502 \u2502 \u2514\u2500\u2500 profile-hadoop.sh \u2502 \u251c\u2500\u2500 hosts \u2502 \u251c\u2500\u2500 motd \u2502 \u251c\u2500\u2500 oozie \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 oozie-env.sh \u2502 \u2502 \u2502 \u2514\u2500\u2500 oozie-site.xml \u2502 \u2502 \u2514\u2500\u2500 profile-oozie.sh \u2502 \u251c\u2500\u2500 spark \u2502 \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2502 \u251c\u2500\u2500 log4j.properties \u2502 \u2502 \u2502 \u2514\u2500\u2500 spark-env.sh \u2502 \u2502 \u2514\u2500\u2500 profile-spark.sh \u2502 \u251c\u2500\u2500 ssh \u2502 \u2502 \u2514\u2500\u2500 config \u2502 \u2514\u2500\u2500 zeppelin \u2502 \u251c\u2500\u2500 config \u2502 \u2502 \u2514\u2500\u2500 zeppelin-env.sh \u2502 \u2514\u2500\u2500 profile-zeppelin.sh \u251c\u2500\u2500 script \u2502 \u251c\u2500\u2500 bootstrap.sh \u2502 \u251c\u2500\u2500 setup_hadoop.sh \u2502 \u251c\u2500\u2500 setup_oozie.sh \u2502 \u251c\u2500\u2500 setup_spark.sh \u2502 \u251c\u2500\u2500 setup_ubuntu.sh \u2502 \u2514\u2500\u2500 setup_zeppelin.sh \u251c\u2500\u2500 Vagrantfile \u2514\u2500\u2500 vagrant_hadoop.sh Import the script source vagrant_hadoop.sh Create and start a Multi Node Hadoop Cluster hadoop-start The first time it might take a while Access the cluster via ssh, check also the /etc/hosts file vagrant ssh master ssh hadoop@172.16.0.10 -i .data/hadoop_rsa # 3 nodes vagrant ssh node-1 ssh hadoop@172.16.0.101 -i .data/hadoop_rsa Destroy the cluster hadoop-destroy For convenience add to the host machine cat hadoop/file/hosts | sudo tee --append /etc/hosts Web UI links NameNode: http://namenode.local:50070 NameNode metrics: http://namenode.local:50070/jmx ResourceManager: http://resource-manager.local:8088 Log Level: http://resource-manager.local:8088/logLevel Web Application Proxy Server: http://web-proxy.local:8100/proxy/application_XXX_0000 MapReduce Job History Server: http://history.local:19888 DataNode/NodeManager (1): http://node-1.local:8042/node DataNode/NodeManager (2): http://node-2.local:8042/node DataNode/NodeManager (3): http://node-3.local:8042/node Spark: http://spark.local:4040 Spark History Server: http://spark-history.local:18080 Zeppelin (*): http://zeppelin.local:8080 Oozie (*): http://oozie.local:11000 (*) Not installed by default","title":"Setup"},{"location":"hadoop/#hdfs-and-mapreduce","text":"HDFS is a distributed file system that provides high-throughput access to application data YARN is a framework for job scheduling and cluster resource management MapReduce is a YARN-based system for parallel processing of large data sets Documentation Hadoop v2.7.6 Untangling Apache Hadoop YARN series","title":"HDFS and MapReduce"},{"location":"hadoop/#admin","text":"HDFS cli # help hdfs # filesystem statistics hdfs dfsadmin -report # filesystem check hdfs fsck / YARN cli # help yarn # list yarn applications yarn application -list # list nodes yarn node -list # view application logs yarn logs -applicationId APPLICATION_ID # kill yarn application yarn application -kill APPLICATION_ID Useful paths # data and logs devops/hadoop/.data/master/hadoop # host /vol/hadoop # guest # (guest) config /usr/local/hadoop/etc/hadoop # (hdfs) map-reduce history /mr-history/history/done_intermediate/hadoop # (hdfs) aggregated app logs /yarn/app/hadoop/logs/application_XXX","title":"Admin"},{"location":"hadoop/#mapreduce-wordcount-job","text":"# build jar on the host machine cd devops/hadoop/example/map-reduce ./gradlew clean build cd devops/hadoop vagrant ssh master # create base directory using hdfs hdfs dfs -mkdir -p /user/ubuntu # create example directory hadoop fs -mkdir -p /user/ubuntu/word-count/input # list directory hadoop fs -ls -h -R / hadoop fs -ls -h -R /user/ubuntu # create sample files echo \"Hello World Bye World\" > file01 echo \"Hello Hadoop Goodbye Hadoop\" > file02 # copy from local to hdfs hadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input hadoop fs -put file02 /user/ubuntu/word-count/input # verify copied files hadoop fs -ls -h -R /user/ubuntu hadoop fs -cat /user/ubuntu/word-count/input/file01 hadoop fs -cat /user/ubuntu/word-count/input/file02 hadoop fs -cat /user/ubuntu/word-count/input/* # run application hadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\ /user/ubuntu/word-count/input \\ /user/ubuntu/word-count/output # check output hadoop fs -cat /user/ubuntu/word-count/output/part-r-00000 # delete directory to run it again hadoop fs -rm -R /user/ubuntu/word-count/output # run sample job in a different queue hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ wordcount \\ -Dmapreduce.job.queuename=root.priority_queue \\ /user/ubuntu/word-count/input \\ /user/ubuntu/word-count/output # well known WARN issue # https://issues.apache.org/jira/browse/HDFS-10429","title":"MapReduce WordCount Job"},{"location":"hadoop/#benchmarking-mapreduce-with-terasort","text":"# generate random data hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ teragen 1000 random-data # run terasort benchmark hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ terasort random-data sorted-data # validate data hadoop jar \\ $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\ teravalidate sorted-data report # useful commands hadoop fs -ls -h -R . hadoop fs -rm -r random-data hadoop fs -cat random-data/part-m-00000 hadoop fs -cat sorted-data/part-r-00000","title":"Benchmarking MapReduce with TeraSort"},{"location":"hadoop/#spark","text":"Spark is an open-source cluster-computing framework Resources Documentation Spark in Action (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i (Book) Big Data Analysis with Scala and Spark (Course) How-to: Tune Your Apache Spark Jobs series Understanding Resource Allocation configurations for a Spark application Apache Spark: Config Cheatsheet Mastering Apache Spark Managing Spark Partitions with Coalesce and Repartition Understanding Apache Spark on YARN Spark application on YARN # start REPL spark-shell pyspark","title":"Spark"},{"location":"hadoop/#interactive-analysis-example","text":"spark-shell # spark shell with yarn spark-shell --master yarn --deploy-mode client # view all configured parameters sc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\")) val licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\") val lineCount = licenceLines.count val isBsd = (line: String) => line.contains(\"BSD\") val bsdLines = licenceLines.filter(isBsd) bsdLines.count bsdLines.foreach(println)","title":"Interactive Analysis example"},{"location":"hadoop/#spark-job-examples","text":"Example local # run SparkPi example spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master local[*] \\ $SPARK_HOME/examples/jars/spark-examples_*.jar 10 # GitHub event documentation # https://developer.github.com/v3/activity/events/types # build jar on the host machine cd devops/hadoop/example/spark sbt clean package cd devops/hadoop vagrant ssh master # sample dataset mkdir -p github-archive && \\ cd $_ && \\ wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\ gunzip -k * # sample line head -n 1 2018-01-01-0.json | jq '.' # run local job spark-submit \\ --class \"com.github.niqdev.App\" \\ --master local[*] \\ /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar Example cluster # run job in YARN cluster-deploy mode spark-submit \\ --class org.apache.spark.examples.SparkPi \\ --master yarn \\ --deploy-mode cluster \\ --driver-memory 2g \\ --executor-memory 1g \\ --executor-cores 3 \\ --queue default \\ $SPARK_HOME/examples/jars/spark-examples*.jar \\ 10 # --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\"","title":"Spark Job examples"},{"location":"hadoop/#zeppelin","text":"Zeppelin is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more Resources Documentation","title":"Zeppelin"},{"location":"hadoop/#setup_1","text":"Install and start Zeppelin # access master node vagrant ssh master # login as root sudo su - # install and init /vagrant/script/setup_zeppelin.sh # start manually (first time only) su --login hadoop /vagrant/script/bootstrap.sh zeppelin","title":"Setup"},{"location":"hadoop/#examples","text":"Learning Spark SQL with Zeppelin # markdown interpreter %md hello # shell interpreter %sh hadoop fs -ls -h -R / Cluster issue: verify to have enough memory with free -m e.g. Error: Cannot allocate memory","title":"Examples"},{"location":"hadoop/#oozie","text":"Oozie is a workflow scheduler system to manage Hadoop jobs Resources Documentation","title":"Oozie"},{"location":"hadoop/#setup_2","text":"Optional PostgreSQL configuration - By default Oozie is configured to use Embedded Derby # access master node vagrant ssh master # install docker curl -fsSL get.docker.com -o get-docker.sh && \\ chmod u+x $_ && \\ ./$_ && \\ sudo usermod -aG docker hadoop # logout and login again to verify docker installation exit vagrant ssh master whoami # hadoop docker ps -a # uncomment PostgreSQL configurations vim devops/hadoop/file/oozie/config/oozie-site.xml # from host vim /vagrant/file/oozie/config/oozie-site.xml # from guest # start postgres on guest machine docker run \\ --detach \\ --name oozie-postgres \\ -p 5432:5432 \\ -e POSTGRES_DB=\"oozie-db\" \\ -e POSTGRES_USER=\"postgres\" \\ -e POSTGRES_PASSWORD=\"password\" \\ postgres # permission issue # https://github.com/docker-library/postgres/issues/116 # --volume /vol/postgres:/var/lib/postgresql/data # access container docker exec -it oozie-postgres bash psql --username=postgres # list all databases \\list \\connect oozie-db # list all tables \\dt # describe table \\d+ wf_jobs # list workflow select * from wf_jobs; Install and start Oozie # access master node vagrant ssh master # login as root sudo su - # build, install and init /vagrant/script/setup_oozie.sh # start oozie manually (first time only) su --login hadoop /vagrant/script/bootstrap.sh oozie It might take a while to build the sources Useful paths # data and logs devops/hadoop/.data/master/oozie # host /vol/oozie # guest # (guest) config /usr/local/oozie/conf # (hdfs) examples /user/hadoop/examples","title":"Setup"},{"location":"hadoop/#examples_1","text":"Run bundled examples within distribution # examples path .data/master/oozie/examples # host /vol/oozie/examples # guest # access master node as hadoop user vagrant ssh master export OOZIE_EXAMPLE_PATH=/vol/oozie/examples export OOZIE_HDFS_PATH=/user/$(whoami)/examples # open map-reduce job.properties vim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties # edit the following properties nameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml jobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml queueName=priority_queue # or default @ fair-scheduler.xml # upload all the examples hadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH # verify uploaded files hadoop fs -ls -h -R /user/$(whoami) # run the map-reduce workflow example oozie job \\ -oozie http://oozie.local:11000/oozie \\ -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\ -run # verify status oozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID # verify result hadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000 # remove all the examples hadoop fs -rm -R $OOZIE_HDFS_PATH","title":"Examples"},{"location":"hadoop/#useful-commands","text":"Workflow requires oozie.wf.application.path property Coordinator requires oozie.coord.application.path property # verify oozie status oozie admin \\ -oozie http://oozie.local:11000/oozie \\ -status # verify workflow or coordinator status oozie job \\ -oozie http://oozie.local:11000/oozie \\ -info JOB_ID \\ -verbose # poll workflow or coordinator status oozie job \\ -oozie http://oozie.local:11000/oozie \\ -poll JOB_ID \\ -interval 10 \\ -timeout 60 \\ -verbose # find running coordinator oozie jobs \\ -oozie http://oozie.local:11000/oozie/ \\ -filter status=RUNNING \\ -jobtype coordinator # suspend|resume|kill coordinator oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ [-suspend|-resume|-kill] \\ XXX-C # re-run coordinator's workflow (action) oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -rerun XXX-C \\ -action 1,2,3,N # kill workflow oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -kill \\ XXX-W # re-run all workflow's actions oozie job \\ -oozie http://oozie.local:11000/oozie/ \\ -rerun \\ XXX-W \\ -Doozie.wf.rerun.failnodes=false","title":"Useful commands"},{"location":"jvm/","text":"JVM Moved to scala-fp","title":"JVM (OLD)"},{"location":"jvm/#jvm","text":"Moved to scala-fp","title":"JVM"},{"location":"kafka/","text":"Kafka Kafka is a distributed streaming platform Resources Documentation Kafka: The Definitive Guide (2017) by Gwen Shapira, Neha Narkhede, Todd Palino (Book) Kafka Streams in Action (2018) by William P. Bejeck Jr. (Book) Kafka: a Distributed Messaging System for Log Processing (Paper) The Internals of Kafka Streams (Book) Gently down the stream (Kid's Book) Schema Registry KafkaProducer javadocs KafkaConsumer javadocs Reactive Kafka Architecture Kafka is a publish/subscribe messaging system often described as a distributed commit log or distributing streaming platform The unit of data is called a message , which is simply an array of bytes and it can have a key used to assign partitions. A batch is a collection of messages, all of which are being produced to the same topic and partition Messages are categorized into topics which are additionally broken down into a number of partitions . Each partition is splitted into segments for storage purposes and each segment is stored in a single data file which contains messages and their offsets Messages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition In order to help brokers quickly locate the message for a given offset, Kafka maintains an index for each partition. The index maps offsets to segment files and positions within the file A stream is considered to be a single topic of data, regardless of the number of partitions Producers , publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly Consumers , subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages i.e. an integer value that continually increases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself Consumers work as part of a consumer group , which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called ownership of the partition by the consumer When a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a rebalance which provide high availability and scalability Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending heartbeats to a Kafka broker designated as the group coordinator You can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer Consumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers commit (track) their offset (position) in each partition to a special __consumer_offsets topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there A single Kafka server is called a broker . The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk Kafka brokers are designed to operate as part of a cluster . A partition is owned by a single broker in the cluster and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the leader replica. Other follower replicas just need to stay in-sync with the leader and replicate all the recent events on time Kafka uses Zookeeper to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an ephemeral node . Kafka uses Zookeeper's ephemeral node feature to elect a controller . The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster Data in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple replicas . Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions A key feature is that of retention . Brokers are configured with a default retention setting for topics, either retaining messages for some period of time or until the topic reaches a certain size in bytes. Once these limits are reached, messages are expired and deleted MirrorMaker is a tool to coordinates multiple clusters or datacenters and replicate data Details The underlying technology of a Kafka topic is a log , which is a file, an append-only, totally ordered sequence of records ordered by time. Topics in Kafka are logs that are segregated by topic name The configuration settings log.dir specifies where Kafka stores log data and each topic maps to a subdirectory. There will be as many subdirectories as there are topic partitions, with a format of partition-name_partition-number . Once the log files reach a certain size (either a number of records or size on disk), or when a configured time difference between message timestamps is reached, the log file is rolled and Kafka appends incoming messages to a new log To manage the increasing size of the logs, Kafka rolls them into segments . The timing of log rolling is based on timestamps embedded in the messages. Kafka rolls a log when a new message arrives, and its timestamp is greater than the timestamp of the first message in the log plus the log.roll.ms . At that point, the log is rolled and a new segment is created as the new active log. The previous active segment is still used to retrieve messages for consumers. Over time, the number of segments will continue to grow, and older segments will need to be deleted to make room for incoming data. To handle the deletion, you can specify how long to retain the segments by log.retention configurations Log compaction ensures that Kafka will always retain at least the last known value for each message key within the log of data for a single topic partition. Instead of taking a coarse-grained approach and deleting entire segments based on time or size, compaction is more fine-grained and deletes old records per key in a log. A log cleaner (a pool of threads) runs in the background, recopying log-segment files and removing records if there's an occurrence later in the log with the same key. To use compaction for a topic, set the log.cleanup.policy=compact property when creating the topic. With a compacted topic, deletion provides a null value for the given key, setting a tombstone marker Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitioning a topic essentially splits the data forwarded to a topic across parallel streams, and it's key for performances and high throughput. Each message has an offset number assigned to it. The order of messages across partitions isn't guaranteed, but the order of messages within each partition is guaranteed Kafka works with data in key/value pairs. If the keys are null , the Kafka producer will write records to partitions chosen in a round-robin fashion, otherwise Kafka uses the formula partition = hashCode(key) % numberOfPartitions to determine to which partition to send the key/value pair to. By using a deterministic approach to select a partition, records with the same key will always be sent to the same partition and in order To determe the correct number of partitions, one of the key considerations is the amount of data flowing into a given topic. More data implies more partitions for higher throughput. On the other hand, increasing the number of partitions increases the number of TCP connections and open file handles. Additionally, how long it takes to process an incoming record in a consumer will also determine throughput. If there is heavyweight processing in a consumer, adding more partitions may help, but ultimately the slower processing will hinder performance Kafka has the notion of leader and follower brokers. In Kafka, for each topic partition, one broker is chosen as the leader for the other brokers (the followers). One of the chief duties of the leader is to assign replication of topic partitions to the follower brokers. When producing messages, Kafka sends the record to the broker that is the leader for the record's partition. Brokers that follow a topic partition consume messages from the topic-partition leader and append those records to their log Kafka uses ZooKeeper to elect the controller broker of the cluster. If the controlling broker fails or becomes unavailable for any reason, ZooKeeper elects a new controller from a set of brokers that are considered to be caught up with the leader (an in-sync replica ISR ). The brokers that make up this set are dynamic, and ZooKeeper recognizes only brokers in this set for election as leader. If a Kafka node dies or is unresponsive (to ZooKeeper heartbeats), all of its assigned partitions (both leader and follower) are reassigned by the controller broker Kafka Streams is a library that allows to perform per-event processing of records, without grouping data in microbatches Kafka Streams is a graph (or topology or Directed Acyclic Graph) of processing nodes or processors that combine to provide powerful and complex stream processing. Each processing node performs its assigned task and then forwards the record to each of its child node. Records (a key/value pair) flow through the graph in a depth-first manner, which implies that there is no need to have backpressure Treating an event stream as inserts, and events with keys as updates, is how to defined the relationship between streams and tables . If a stream of events is as a log, a stream of updates is as a changelog. Both a log and a changelog represent incoming records appended to the end of a file. In a log there are all the records; but in a changelog, there are only the latest record for any given key A KTable is often described as being a materialized view of a KStream , a view of a stream is nothing but a per-key aggregation Setup Requirements Base docker image ZooKeeper docker image Build devops/kafka image # change path cd devops/kafka # build image docker build -t devops/kafka . # create network docker network create --driver bridge my_network docker network ls docker network inspect my_network # start temporary zookeeper container [host:container] docker run --rm \\ --name zookeeper \\ -p 12181:2181 \\ --network=my_network \\ devops/zookeeper # access container docker exec -it zookeeper bash # start temporary kafka container [host:container] docker run --rm \\ --name kafka \\ -p 19092:9092 \\ --network=my_network \\ -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\ devops/kafka # access container docker exec -it kafka bash # paths /opt/kafka /opt/kafka/logs /var/lib/kafka/data # supervisor logs /var/log/kafka /var/log/connect tail -F /var/log/kafka/stdout less +G /var/log/connect/stdout Alternatively use docker-compose # change path cd devops/kafka # build base image docker build -t devops/base ../base # build + start zookeeper and kafka docker-compose up # access container docker exec -it devops-zookeeper bash docker exec -it devops-kafka bash How-To Kafka docker exec -it devops-kafka bash # create topic kafka-topics.sh --zookeeper zookeeper:2181 \\ --create --if-not-exists --replication-factor 1 --partitions 1 --topic test # view topic kafka-topics.sh --zookeeper zookeeper:2181 --list kafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test kafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions kafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions # produce kafka-console-producer.sh --broker-list kafka:9092 --topic test # util kafkacat -P -b 0 -t test # consume kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning # util kafkacat -C -b 0 -t test # list consumers kafka-consumer-groups.sh --bootstrap-server kafka:9092 --list # view lag (GROUP_NAME from previous command) kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME # delete kafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test # verify log segment and index kafka-run-class.sh kafka.tools.DumpLogSegments \\ --files /var/lib/kafka/data/test-0/00000000000000000000.log kafka-run-class.sh kafka.tools.DumpLogSegments \\ --index-sanity-check \\ --files /var/lib/kafka/data/test-0/00000000000000000000.index # inspect __consumer_offsets kafka-console-consumer.sh --bootstrap-server kafka:9092 \\ --topic __consumer_offsets \\ --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\ --max-messages 1 Connect docker exec -it devops-kafka bash # verify connect http :8083 http :8083/connector-plugins # write file to topic http POST :8083/connectors \\ name=load-kafka-config \\ config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}' # verify topic kafka-console-consumer.sh --bootstrap-server=kafka:9092 \\ --topic kafka-config-topic --from-beginning # write topic to file http POST :8083/connectors \\ name=dump-kafka-config \\ config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}' # verify file vim /tmp/copy-of-server-properties # manage connectors http :8083/connectors http DELETE :8083/connectors/dump-kafka-config ZooKeeper docker exec -it devops-zookeeper bash # start cli zkCli.sh # view ephemeral nodes ls /brokers/ids get /brokers/ids/0 # view topics ls /brokers/topics get /brokers/topics/test Schema Registry # docker-hub images docker-compose -f kafka/docker-compose-hub.yml up docker exec -it devops-schema-registry bash # register new schema http -v POST :8081/subjects/ExampleSchema/versions \\ Accept:application/vnd.schemaregistry.v1+json \\ schema='{\"type\":\"string\"}' # list subjects and schema http -v :8081/subjects \\ Accept:application/vnd.schemaregistry.v1+json http -v :8081/subjects/ExampleSchema/versions \\ Accept:application/vnd.schemaregistry.v1+json http -v :8081/subjects/ExampleSchema/versions/1 \\ Accept:application/vnd.schemaregistry.v1+json # ui [mac|linux] [open|xdg-open] http://localhost:8082","title":"Kafka"},{"location":"kafka/#kafka","text":"Kafka is a distributed streaming platform Resources Documentation Kafka: The Definitive Guide (2017) by Gwen Shapira, Neha Narkhede, Todd Palino (Book) Kafka Streams in Action (2018) by William P. Bejeck Jr. (Book) Kafka: a Distributed Messaging System for Log Processing (Paper) The Internals of Kafka Streams (Book) Gently down the stream (Kid's Book) Schema Registry KafkaProducer javadocs KafkaConsumer javadocs Reactive Kafka","title":"Kafka"},{"location":"kafka/#architecture","text":"Kafka is a publish/subscribe messaging system often described as a distributed commit log or distributing streaming platform The unit of data is called a message , which is simply an array of bytes and it can have a key used to assign partitions. A batch is a collection of messages, all of which are being produced to the same topic and partition Messages are categorized into topics which are additionally broken down into a number of partitions . Each partition is splitted into segments for storage purposes and each segment is stored in a single data file which contains messages and their offsets Messages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition In order to help brokers quickly locate the message for a given offset, Kafka maintains an index for each partition. The index maps offsets to segment files and positions within the file A stream is considered to be a single topic of data, regardless of the number of partitions Producers , publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly Consumers , subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the offset of messages i.e. an integer value that continually increases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself Consumers work as part of a consumer group , which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called ownership of the partition by the consumer When a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a rebalance which provide high availability and scalability Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending heartbeats to a Kafka broker designated as the group coordinator You can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer Consumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers commit (track) their offset (position) in each partition to a special __consumer_offsets topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there A single Kafka server is called a broker . The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk Kafka brokers are designed to operate as part of a cluster . A partition is owned by a single broker in the cluster and that broker is called the leader of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the leader replica. Other follower replicas just need to stay in-sync with the leader and replicate all the recent events on time Kafka uses Zookeeper to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an ephemeral node . Kafka uses Zookeeper's ephemeral node feature to elect a controller . The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster Data in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple replicas . Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions A key feature is that of retention . Brokers are configured with a default retention setting for topics, either retaining messages for some period of time or until the topic reaches a certain size in bytes. Once these limits are reached, messages are expired and deleted MirrorMaker is a tool to coordinates multiple clusters or datacenters and replicate data","title":"Architecture"},{"location":"kafka/#details","text":"The underlying technology of a Kafka topic is a log , which is a file, an append-only, totally ordered sequence of records ordered by time. Topics in Kafka are logs that are segregated by topic name The configuration settings log.dir specifies where Kafka stores log data and each topic maps to a subdirectory. There will be as many subdirectories as there are topic partitions, with a format of partition-name_partition-number . Once the log files reach a certain size (either a number of records or size on disk), or when a configured time difference between message timestamps is reached, the log file is rolled and Kafka appends incoming messages to a new log To manage the increasing size of the logs, Kafka rolls them into segments . The timing of log rolling is based on timestamps embedded in the messages. Kafka rolls a log when a new message arrives, and its timestamp is greater than the timestamp of the first message in the log plus the log.roll.ms . At that point, the log is rolled and a new segment is created as the new active log. The previous active segment is still used to retrieve messages for consumers. Over time, the number of segments will continue to grow, and older segments will need to be deleted to make room for incoming data. To handle the deletion, you can specify how long to retain the segments by log.retention configurations Log compaction ensures that Kafka will always retain at least the last known value for each message key within the log of data for a single topic partition. Instead of taking a coarse-grained approach and deleting entire segments based on time or size, compaction is more fine-grained and deletes old records per key in a log. A log cleaner (a pool of threads) runs in the background, recopying log-segment files and removing records if there's an occurrence later in the log with the same key. To use compaction for a topic, set the log.cleanup.policy=compact property when creating the topic. With a compacted topic, deletion provides a null value for the given key, setting a tombstone marker Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitioning a topic essentially splits the data forwarded to a topic across parallel streams, and it's key for performances and high throughput. Each message has an offset number assigned to it. The order of messages across partitions isn't guaranteed, but the order of messages within each partition is guaranteed Kafka works with data in key/value pairs. If the keys are null , the Kafka producer will write records to partitions chosen in a round-robin fashion, otherwise Kafka uses the formula partition = hashCode(key) % numberOfPartitions to determine to which partition to send the key/value pair to. By using a deterministic approach to select a partition, records with the same key will always be sent to the same partition and in order To determe the correct number of partitions, one of the key considerations is the amount of data flowing into a given topic. More data implies more partitions for higher throughput. On the other hand, increasing the number of partitions increases the number of TCP connections and open file handles. Additionally, how long it takes to process an incoming record in a consumer will also determine throughput. If there is heavyweight processing in a consumer, adding more partitions may help, but ultimately the slower processing will hinder performance Kafka has the notion of leader and follower brokers. In Kafka, for each topic partition, one broker is chosen as the leader for the other brokers (the followers). One of the chief duties of the leader is to assign replication of topic partitions to the follower brokers. When producing messages, Kafka sends the record to the broker that is the leader for the record's partition. Brokers that follow a topic partition consume messages from the topic-partition leader and append those records to their log Kafka uses ZooKeeper to elect the controller broker of the cluster. If the controlling broker fails or becomes unavailable for any reason, ZooKeeper elects a new controller from a set of brokers that are considered to be caught up with the leader (an in-sync replica ISR ). The brokers that make up this set are dynamic, and ZooKeeper recognizes only brokers in this set for election as leader. If a Kafka node dies or is unresponsive (to ZooKeeper heartbeats), all of its assigned partitions (both leader and follower) are reassigned by the controller broker Kafka Streams is a library that allows to perform per-event processing of records, without grouping data in microbatches Kafka Streams is a graph (or topology or Directed Acyclic Graph) of processing nodes or processors that combine to provide powerful and complex stream processing. Each processing node performs its assigned task and then forwards the record to each of its child node. Records (a key/value pair) flow through the graph in a depth-first manner, which implies that there is no need to have backpressure Treating an event stream as inserts, and events with keys as updates, is how to defined the relationship between streams and tables . If a stream of events is as a log, a stream of updates is as a changelog. Both a log and a changelog represent incoming records appended to the end of a file. In a log there are all the records; but in a changelog, there are only the latest record for any given key A KTable is often described as being a materialized view of a KStream , a view of a stream is nothing but a per-key aggregation","title":"Details"},{"location":"kafka/#setup","text":"Requirements Base docker image ZooKeeper docker image Build devops/kafka image # change path cd devops/kafka # build image docker build -t devops/kafka . # create network docker network create --driver bridge my_network docker network ls docker network inspect my_network # start temporary zookeeper container [host:container] docker run --rm \\ --name zookeeper \\ -p 12181:2181 \\ --network=my_network \\ devops/zookeeper # access container docker exec -it zookeeper bash # start temporary kafka container [host:container] docker run --rm \\ --name kafka \\ -p 19092:9092 \\ --network=my_network \\ -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\ devops/kafka # access container docker exec -it kafka bash # paths /opt/kafka /opt/kafka/logs /var/lib/kafka/data # supervisor logs /var/log/kafka /var/log/connect tail -F /var/log/kafka/stdout less +G /var/log/connect/stdout Alternatively use docker-compose # change path cd devops/kafka # build base image docker build -t devops/base ../base # build + start zookeeper and kafka docker-compose up # access container docker exec -it devops-zookeeper bash docker exec -it devops-kafka bash","title":"Setup"},{"location":"kafka/#how-to","text":"Kafka docker exec -it devops-kafka bash # create topic kafka-topics.sh --zookeeper zookeeper:2181 \\ --create --if-not-exists --replication-factor 1 --partitions 1 --topic test # view topic kafka-topics.sh --zookeeper zookeeper:2181 --list kafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test kafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions kafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions # produce kafka-console-producer.sh --broker-list kafka:9092 --topic test # util kafkacat -P -b 0 -t test # consume kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning # util kafkacat -C -b 0 -t test # list consumers kafka-consumer-groups.sh --bootstrap-server kafka:9092 --list # view lag (GROUP_NAME from previous command) kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME # delete kafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test # verify log segment and index kafka-run-class.sh kafka.tools.DumpLogSegments \\ --files /var/lib/kafka/data/test-0/00000000000000000000.log kafka-run-class.sh kafka.tools.DumpLogSegments \\ --index-sanity-check \\ --files /var/lib/kafka/data/test-0/00000000000000000000.index # inspect __consumer_offsets kafka-console-consumer.sh --bootstrap-server kafka:9092 \\ --topic __consumer_offsets \\ --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\ --max-messages 1 Connect docker exec -it devops-kafka bash # verify connect http :8083 http :8083/connector-plugins # write file to topic http POST :8083/connectors \\ name=load-kafka-config \\ config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}' # verify topic kafka-console-consumer.sh --bootstrap-server=kafka:9092 \\ --topic kafka-config-topic --from-beginning # write topic to file http POST :8083/connectors \\ name=dump-kafka-config \\ config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}' # verify file vim /tmp/copy-of-server-properties # manage connectors http :8083/connectors http DELETE :8083/connectors/dump-kafka-config ZooKeeper docker exec -it devops-zookeeper bash # start cli zkCli.sh # view ephemeral nodes ls /brokers/ids get /brokers/ids/0 # view topics ls /brokers/topics get /brokers/topics/test Schema Registry # docker-hub images docker-compose -f kafka/docker-compose-hub.yml up docker exec -it devops-schema-registry bash # register new schema http -v POST :8081/subjects/ExampleSchema/versions \\ Accept:application/vnd.schemaregistry.v1+json \\ schema='{\"type\":\"string\"}' # list subjects and schema http -v :8081/subjects \\ Accept:application/vnd.schemaregistry.v1+json http -v :8081/subjects/ExampleSchema/versions \\ Accept:application/vnd.schemaregistry.v1+json http -v :8081/subjects/ExampleSchema/versions/1 \\ Accept:application/vnd.schemaregistry.v1+json # ui [mac|linux] [open|xdg-open] http://localhost:8082","title":"How-To"},{"location":"kubernetes/","text":"Kubernetes Kubernetes is a system for automating deployment, scaling, and management of containerized applications Resources Documentation Offical Blog Kubernetes in Action (2017) by Marko Luk\u0161a (Book) YouTube channels: Kubernetes and Cloud Native Computing Foundation Blogs Running akka-cluster on Kubernetes Kubernetes: The Surprisingly Affordable Platform for Personal Projects Kubernetes from scratch to AWS with Terraform and Ansible Prometheus and Kubernetes: A Perfect Match Inside of Kubernetes Controller Exploring Kubernetes Operator Pattern Kubernetes: Make your services faster by removing CPU limits Optimizing Kubernetes Resource Requests/Limits for Cost-Efficiency and Latency What are Quality of Service (QoS) Classes in Kubernetes How to detect Kubernetes overspending by measuring idle costs Architecture At the hardware level, a Kubernetes cluster node can be a master node, which hosts the Kubernetes Control Plane that manages the state of the cluster The Kubernetes API Server to communicate with the cluster and the only component that talks to etcd directly The Scheduler , which schedules apps (assigns a worker node to each deployable component) The Controller Manager , which performs cluster-level functions, such as replicating components, keeping track of worker nodes handling node failures, and so on etcd , a fast, distributed, and consistent key-value store which is the only place where cluster state and metadata are stored a worker nodes that run containerized applications Docker, rkt, or another container runtime , which runs containers The Kubelet , which talks to the API server and manages containers on its node The Kubernetes Service Proxy (kube-proxy) , which load-balances network traffic between application components some of the add-on components are Kubernetes DNS server Dashboard Ingress controller Heapster Container Network Interface network plugin To run an application in Kubernetes, it needs to be packaged into one or more container images, those images need to be pushed to an image registry, and then a description of the app posted to the Kubernetes API Server When the API server processes the app's description, the Scheduler schedules (pods are run immediately) the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers Once the application is running, Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided To allow clients to easily find containers that provide a specific service, it's possible to tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster User interacts with the cluster through the kubectl command line client, which issues REST requests to the Kubernetes API server running on the master node One of the most fundamental Kubernetes principles is that instead of telling Kubernetes exactly what actions it should perform, you're only declaratively changing the desired state of the system and letting Kubernetes examine the current actual state and reconcile it with the desired state Resource definition example apiVersion: v1 # type of resource kind: Pod # includes the name, namespace, labels, and other information about the pod metadata: ... # contains the actual description of the pod's contents, # such as the pod's containers, volumes, and other data spec: ... # contains the current information about the running pod, # such as what condition the pod is in, the description and status of each container, # and the pod's internal IP and other basic info status: ... A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors and a resource can have more than one label Annotations are key-value pairs like labels, but they aren't meant to hold identifying information and can hold up to 256 KB Using multiple namespaces allows to split complex systems with numerous components into smaller distinct groups and resource names only need to be unique within a namespace A pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s). Each pod is like a separate logical machine with its own IP, hostname, processes, and so on, running a single application A ReplicationController is a resource that ensures its pods are always kept running and an exact number of pods always matches its label selector, even if a node disappears. It's made by a label selector , which determines what pods are in the ReplicationController's scope a replica count , which specifies the desired number of pods that should be running a pod template , which is used when creating new pod replicas A ReplicaSet behaves exactly like a ReplicationController (old), but it has more expressive pod selectors A DaemonSet is mostly for specific case like infrastructure-related pods that perform system-level operations in which a pod must run on each and every node in the cluster and each node needs to run exactly one instance of the pod, for example log collector and resource monitor A Job resource allows to run a pod whose container isn't restarted when the process running inside finishes successfully. A cron job in Kubernetes is configured by creating a CronJob resource A Service represents a static location for a group of one or more pods that all provide the same service. Requests coming to the IP and port of the service will be forwarded/load-balanced to the IP and port of one of the pods belonging to the service at that moment ExternalName type, a service that serves as an alias for an external service NodePort type, each cluster node opens a port on the node itself and redirects traffic received on that port to the underlying service LoadBalancer type (extension of NodePort), makes the service accessible through a dedicated load balancer which usually is supported and automatically provisioned by the cloud infrastructure A headless service still provides load balancing across pods, but through the DNS round-robin mechanism instead of through the service proxy, because DNS returns the pods' IPs, clients connect directly to the pods, instead of through the service proxy. Setting the clusterIP field in a service spec to None makes the service headless An Endpoints resource is a list of IP addresses and ports exposing a service. The Endpoints object needs to have the same name as the service and contain the list of target IP addresses and ports for the service An Ingress operates at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity. LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one The Kubelet on the node hosting the pod can check if a container is still alive through liveness probes using httpGet , tcpSocket or exec . Exit code is a sum of 128 + N e.g. 137 = 128 + 9 (SIGKILL) or 143 = 128 + 15 (SIGTERM). Always remember to set an initial delay initialDelaySeconds The readiness probes is invoked periodically and determines whether the specific pod should receive client requests or not. Liveness probes keep pods healthy by killing off unhealthy containers and replacing them with new, healthy ones, whereas readiness probes make sure that only pods that are ready to serve requests receive them and this is mostly necessary during container start up A volume is a component of a pod and not a standalone object, it cannot be created or deleted on its own. A volume is available to all containers in the pod, but it must be mounted in each container that needs to access it emptyDir , a simple empty directory used for storing transient data hostPath , used for mounting directories from the worker node's filesystem into the pod gitRepo , a volume initialized by checking out the contents of a Git repository nfs , an NFS share mounted into the pod gcePersistentDisk (Google Compute Engine Persistent Disk), awsElasticBlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk (Microsoft Azure Disk Volume), used for mounting cloud provider-specific storage cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphereVolume, photonPersistentDisk, scaleIO , used for mounting other types of network storage configMap, secret, downwardAPI , a special types of volumes used to expose certain Kubernetes resources and cluster information to the pod persistentVolumeClaim , a way to use a pre- or dynamically provisioned persistent storage An app can be configured by passing command-line arguments to containers with command and args setting custom environment variables for each container of a pod mounting configuration files into containers through a special type of volume The contents of a ConfigMap are passed to containers as either environment variables or as files in a volume while on the nodes Secrets are always stored in memory and never written to physical storage (maximum size of a Secret is limited to 1MB) The Downward API enables you to expose the pod's own metadata to the processes running inside that pod (it isn't a REST endpoint) Rolling update means replace pods step by step slowly scaling down the previous version and scaling up the new one A Deployment is a higher-level resource meant for deploying applications and updating them declaratively, instead of doing it through a ReplicationController or a ReplicaSet, which are both considered lower-level concepts. A Deployment doesn't manage pods directly, instead it creates a new ReplicaSet which is scaled up slowly, while the previous ReplicaSet is scaled down to zero. See also minReadySeconds , maxSurge and maxUnavailable properties A StatefulSet makes sure pods are rescheduled in such a way that they retain their identity and state. StatefulSets were initially called PetSets, that name comes from the pets vs. cattle analogy . Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which is then used to derive the pod's name and hostname, and to attach stable storage to the pod. A StatefulSet requires to create a corresponding governing headless Service ( clusterIP=None ) that's used to provide the actual network identity to each pod, in this case each pod gets its own DNS entry. The new pod isn't necessarily scheduled to the same node. Scaling the StatefulSet creates a new pod instance with the next unused ordinal index. Scaling down a StatefulSet always removes the instances with the highest ordinal index first. StatefulSets don't delete PersistentVolumeClaims when scaling down and they reattach them when scaling back up Clients watch for changes by opening an HTTP connection to the API server. Through this connection, the client will then receive a stream of modifications to the watched objects The Scheduler waits for newly created pods through the API server's watch mechanism and assign a node to each new pod that doesn't already have the node set. The Scheduler doesn't instruct the selected node (or the Kubelet running on that node) to run the pod. All the Scheduler does is update the pod definition through the API server. The API server then notifies the Kubelet that the pod has been scheduled. As soon as the Kubelet on the target node sees the pod has been scheduled to its node, it creates and runs the pod's containers The single Controller Manager process currently combines a multitude of controllers performing various reconciliation tasks Replication Manager (a controller for ReplicationController resources) ReplicaSet, DaemonSet, and Job controllers Deployment controller StatefulSet controller Node controller Service controller Endpoints controller Namespace controller PersistentVolume controller Others Controllers do many different things, but they all watch the API server for changes to resources and perform operations for each change. In general, controllers run a reconciliation loop, which reconciles the actual state with the desired state (specified in the resource's spec section) and writes the new actual state to the resource's status section The Kubelet is the component responsible for everything running on a worker node. Its initial job is to register the node it's running on by creating a Node resource in the API server. Then it needs to continuously monitor the API server for Pods that have been scheduled to the node, and start the pod's containers. It does this by telling the configured container runtime (i.e. Docker) to run a container from a specific container image. The Kubelet then constantly monitors running containers and reports their status, events, and resource consumption to the API server. The Kubelet is also the component that runs the container liveness probes, restarting containers when the probes fail. Lastly, it terminates containers when their Pod is deleted from the API server and notifies the server that the pod has terminated Every worker node also runs the kube-proxy (Service Proxy), whose purpose is to make sure clients can connect to the services you define through the Kubernetes API and performs load balancing across those pods. The current, implementation only uses iptables rules to redirect packets to a randomly selected backend pod without passing them through an actual proxy server Every pod is associated with a ServiceAccount , which represents the identity of the app running in the pod. The token file /var/run/secrets/kubernetes.io/serviceaccount/token , which is mounted into each container's filesystem through a secret volume, holds the ServiceAccount's authentication token used to connect to the API server RBAC (role-based access control) plugin prevents unauthorized users from viewing or modifying the cluster state. Roles are managed by Roles , ClusterRoles , RoleBindings and ClusterRoleBindings resources. Cluster level resources are not namespaced. Roles define what can be done, while bindings define who can do it Setup Requirements Minikube VirtualBox Local cluster # verify installation minikube version # lifecycle minikube start --vm-driver=virtualbox minikube stop minikube delete # dashboard export NO_PROXY=localhost,127.0.0.1,$(minikube ip) minikube dashboard # access minikube ssh docker ps -a # reuse the minikube's built-in docker daemon eval $(minikube docker-env) # access NodePort services minikube service <SERVICE_NAME> [-n <NAMESPACE>] # list addons minikube addons list # enable addon minikube addons enable <ADDON_NAME> # (?) swagger minikube start --extra-config=apiserver.Features.EnableSwaggerUI=true kubectl proxy open http://localhost:8080/swagger-ui Basic # verify installation kubectl version # cluster info kubectl cluster-info kubectl get nodes kubectl describe nodes kubectl config view # health status of each control plane component kubectl get componentstatuses # namespace kubectl create namespace <NAMESPACE_NAME> kubectl get namespaces kubectl config view | grep namespace kubectl delete namespace <NAMESPACE_NAME> # current namespace kubectl config current-context # switch namespace kubectl config set-context $(kubectl config current-context) --namespace=<NAMESPACE_NAME> # create/update resources from file kubectl create -f <FILE_NAME>.yaml kubectl apply -f <FILE_NAME>.yaml # explain fields kubectl explain pod kubectl explain service.spec # edit resource (vim) kubectl edit pod <POD_NAME> # verbosity kubectl get pods --v 6 # access etcd on minikube minikube ssh docker run \\ --rm \\ -p 12379:2379 \\ -p 12380:2380 \\ --mount type=bind,source=/tmp/etcd-data.tmp,destination=/etcd-data \\ --name etcd-gcr-v3.3.10 \\ gcr.io/etcd-development/etcd:v3.3.10 \\ /usr/local/bin/etcd \\ --name s1 \\ --data-dir /etcd-data \\ --listen-client-urls http://0.0.0.0:12379 \\ --advertise-client-urls http://0.0.0.0:12379 \\ --listen-peer-urls http://0.0.0.0:12380 \\ --initial-advertise-peer-urls http://0.0.0.0:12380 \\ --initial-cluster s1=http://0.0.0.0:12380 \\ --initial-cluster-token tkn \\ --initial-cluster-state new docker exec etcd-gcr-v3.3.10 /bin/sh -c \"ETCDCTL_API=3 /usr/local/bin/etcdctl version\" docker exec etcd-gcr-v3.3.10 /bin/sh -c \"ETCDCTL_API=3 /usr/local/bin/etcdctl get /registry --endpoints=127.0.0.1:12379\" # watch cluster events kubectl get events --watch Simple deployment # deploy demo app kubectl run kubernetes-bootcamp \\ --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\ --port=8080 \\ --labels='app=kubernetes-bootcamp' # update app kubectl set image deployments/kubernetes-bootcamp \\ kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 # verify update kubectl rollout status deployments/kubernetes-bootcamp kubectl rollout history deployments/kubernetes-bootcamp # undo latest deployment kubectl rollout undo deployments/kubernetes-bootcamp # undo deployment to revision N kubectl rollout undo deployment <DEPLOYMENT_NAME> --to-revision=N # pause/resume deployment kubectl rollout pause deployment <DEPLOYMENT_NAME> kubectl rollout resume deployment <DEPLOYMENT_NAME> # list deployments kubectl get deployments kubectl describe deployment # create deployment from file (records the command in the revision history) kubectl create -f <FILE_NAME>.yaml --record # manually slowdown a rolling update kubectl patch deployment <DEPLOYMENT_NAME> -p '{\"spec\": {\"minReadySeconds\": 10}}' Pod and Container # proxy cluster (open in 2nd terminal) kubectl proxy # pod name export POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo POD_NAME=$POD_NAME # verify proxy http :8001/version http :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/ # view logs kubectl logs $POD_NAME kubectl logs <POD_NAME> -c <CONTAINER_NAME> kubectl logs <POD_NAME> --previous # execute command on container kubectl exec $POD_NAME printenv kubectl exec $POD_NAME ls -- -la # access container kubectl exec -it $POD_NAME bash # verify label kubectl describe pods $POD_NAME # list containers inside pods kubectl describe pods # list pods kubectl get pods # list pods and nodes kubectl get pods -o wide # list pods with labels kubectl get po --show-labels kubectl get pods -L <LABEL_KEY> # filter with equality-based labels kubectl get pods -l app=kubernetes-bootcamp kubectl get pods -l app kubectl get pods -l '!app' # filter with set-based labels kubectl get pods -l 'app in (kubernetes-bootcamp)' # add/update labels manually kubectl label po <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> kubectl label po <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> --overwrite # annotate kubectl annotate pod <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> # print definition kubectl get pod <POD_NAME> -o json kubectl get pod <POD_NAME> -o yaml # output json kubectl get pod <POD_NAME> -o json | jq '.metadata' # output json kubectl get pod <POD_NAME> -o yaml | yq '.metadata.annotations' # output yaml kubectl get pod <POD_NAME> -o yaml | yq -y '.metadata.annotations' # by namespace kubectl get ns kubectl get pod --namespace kube-system kubectl get po --all-namespaces # watch for changes kubectl get pods --watch kubectl get pods -o yaml --watch # (debug) forward a local network port to a port in the pod (without service) kubectl port-forward <POD_NAME> <LOCAL_PORT>:<POD_PORT> # delete (sends SIGTERM to containers and waits 30 seconds, otherwise sends SIGKILL) kubectl delete po <POD_NAME> kubectl delete pod -l <LABEL_KEY>=<LABEL_VALUE> kubectl delete po --all Service # list services kubectl get svc kubectl get services # create service kubectl expose deployment/kubernetes-bootcamp \\ --type=\"NodePort\" \\ --port 8080 # service info kubectl describe services/kubernetes-bootcamp # expose service export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT # verify service curl $(minikube ip):$NODE_PORT # add 4 replicas kubectl scale deployments/kubernetes-bootcamp --replicas=4 # info kubectl get pod,deployment,service kubectl get pods -o wide kubectl describe deployments/kubernetes-bootcamp # cleanup kubectl delete deployment,service kubernetes-bootcamp # all (means all resource types) # --all (means all resource instances) kubectl delete all --all Other # replication controller kubectl get rc kubectl get replicationcontroller # replica set kubectl get rs kubectl get replicaset # jobs kubectl get jobs # ingress kubectl get ingress # persistent volume, claim and storage class kubectl get pv kubectl get pvc kubectl get sc # jsonpath example kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # create config map kubectl create configmap my-config \\ --from-literal=my-key-1=my-value-1 \\ --from-literal=my-key-2=my-value-2 # print config-map kubectl get configmap my-config -o yaml # secrets kubectl get secrets kubectl describe secrets kubectl exec <POD_NAME> ls /var/run/secrets/kubernetes.io/serviceaccount/ # create secrets (generic|tls|docker-registry) echo bar > foo.secure kubectl create secret generic my-secret --from-file=foo.secure # contents are shown as Base64-encoded strings kubectl get secret my-secret -o yaml # prints \"bar\" echo YmFyCg== | base64 -D # rbac kubectl get clusterroles kubectl get clusterrolebindings kubectl get roles kubectl get rolebindings # access api server from local machine kubectl proxy http :8001 http :8001/api/v1/pods # access api server from a container # (create) kubectl run hello-curl \\ --image=tutum/curl \\ --command -- \"sleep\" \"9999999\" # (exec) kubectl exec -it hello-curl-XXX bash # (verify envs) printenv # (missing certificate) curl -k https://kubernetes # (list secrets) ls -laht /var/run/secrets/kubernetes.io/serviceaccount/ # (specify certificate) curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes # (specify token, namespace and default certificate) or \"https://kubernetes.default/api/v1\" export KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) export KUBE_NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace) export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt curl -v \\ -H \"Authorization: Bearer $KUBE_TOKEN\" \\ https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/$KUBE_NS/pods # @see also ambassador container pattern based on kubectl-proxy # api server proxy to invoke endpoint on resource kubectl proxy curl localhost:8001/api/v1/namespaces/<NAMESPACE_NAME>/<pods|services|...>/<RESOURCE_NAME>/proxy/<PATH> # temporary command kubectl run \\ -it srvlookup \\ --image=tutum/dnsutils \\ --rm \\ --restart=Never \\ -- dig <SERVICE_NAME> kubia.default.svc.cluster.local # temporary container kubectl run \\ -it phusion \\ --image=phusion/baseimage:latest \\ --rm \\ --restart=Never \\ bash Alternative ways of modify resources # opens the object's manifest in default editor kubectl edit # modifies individual properties of an object kubectl patch # creates or modifies the object by applying property values from a full YAML or JSON file kubectl apply # same as apply but only if not exists kubectl create # same as apply but only if exists kubectl replace # changes the container image kubectl set image Helm Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file A Repository is the place where charts can be collected and shared A Release is an instance of a chart running in a Kubernetes cluster Resources Documentation The missing CI/CD Kubernetes component: Helm package manager Commands # linux sudo snap install helm --classic # mac brew install kubernetes-helm # setup client only helm init --client-only # search helm search <CHART> helm search postgresql # info helm inspect <CHART> helm inspect stable/postgresql helm inspect values stable/postgresql helm get values <KEY> helm status <CHART> helm list helm list --all # install public chart [install|upgrade|rollback|delete] helm install <CHART> helm install -f <CONFIG_OVERRIDE>.yaml <CHART> helm delete <CHART> # repository helm repo list helm repo add <NAME> https://<DOMAIN>/<PATH> helm repo update # custom chart helm create my-chart helm lint my-chart helm package my-chart helm install ./my-chart helm install --dry-run --debug ./my-chart # plugin $(helm home)/plugins helm plugin install <PATH|URL>","title":"Kubernetes"},{"location":"kubernetes/#kubernetes","text":"Kubernetes is a system for automating deployment, scaling, and management of containerized applications Resources Documentation Offical Blog Kubernetes in Action (2017) by Marko Luk\u0161a (Book) YouTube channels: Kubernetes and Cloud Native Computing Foundation Blogs Running akka-cluster on Kubernetes Kubernetes: The Surprisingly Affordable Platform for Personal Projects Kubernetes from scratch to AWS with Terraform and Ansible Prometheus and Kubernetes: A Perfect Match Inside of Kubernetes Controller Exploring Kubernetes Operator Pattern Kubernetes: Make your services faster by removing CPU limits Optimizing Kubernetes Resource Requests/Limits for Cost-Efficiency and Latency What are Quality of Service (QoS) Classes in Kubernetes How to detect Kubernetes overspending by measuring idle costs","title":"Kubernetes"},{"location":"kubernetes/#architecture","text":"At the hardware level, a Kubernetes cluster node can be a master node, which hosts the Kubernetes Control Plane that manages the state of the cluster The Kubernetes API Server to communicate with the cluster and the only component that talks to etcd directly The Scheduler , which schedules apps (assigns a worker node to each deployable component) The Controller Manager , which performs cluster-level functions, such as replicating components, keeping track of worker nodes handling node failures, and so on etcd , a fast, distributed, and consistent key-value store which is the only place where cluster state and metadata are stored a worker nodes that run containerized applications Docker, rkt, or another container runtime , which runs containers The Kubelet , which talks to the API server and manages containers on its node The Kubernetes Service Proxy (kube-proxy) , which load-balances network traffic between application components some of the add-on components are Kubernetes DNS server Dashboard Ingress controller Heapster Container Network Interface network plugin To run an application in Kubernetes, it needs to be packaged into one or more container images, those images need to be pushed to an image registry, and then a description of the app posted to the Kubernetes API Server When the API server processes the app's description, the Scheduler schedules (pods are run immediately) the specified groups of containers onto the available worker nodes based on computational resources required by each group and the unallocated resources on each node at that moment The Kubelet on those nodes then instructs the Container Runtime (Docker, for example) to pull the required container images and run the containers Once the application is running, Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided To allow clients to easily find containers that provide a specific service, it's possible to tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications running in the cluster User interacts with the cluster through the kubectl command line client, which issues REST requests to the Kubernetes API server running on the master node One of the most fundamental Kubernetes principles is that instead of telling Kubernetes exactly what actions it should perform, you're only declaratively changing the desired state of the system and letting Kubernetes examine the current actual state and reconcile it with the desired state Resource definition example apiVersion: v1 # type of resource kind: Pod # includes the name, namespace, labels, and other information about the pod metadata: ... # contains the actual description of the pod's contents, # such as the pod's containers, volumes, and other data spec: ... # contains the current information about the running pod, # such as what condition the pod is in, the description and status of each container, # and the pod's internal IP and other basic info status: ... A label is an arbitrary key-value pair you attach to a resource, which is then utilized when selecting resources using label selectors and a resource can have more than one label Annotations are key-value pairs like labels, but they aren't meant to hold identifying information and can hold up to 256 KB Using multiple namespaces allows to split complex systems with numerous components into smaller distinct groups and resource names only need to be unique within a namespace A pod is a group of one or more tightly related containers that will always run together on the same worker node and in the same Linux namespace(s). Each pod is like a separate logical machine with its own IP, hostname, processes, and so on, running a single application A ReplicationController is a resource that ensures its pods are always kept running and an exact number of pods always matches its label selector, even if a node disappears. It's made by a label selector , which determines what pods are in the ReplicationController's scope a replica count , which specifies the desired number of pods that should be running a pod template , which is used when creating new pod replicas A ReplicaSet behaves exactly like a ReplicationController (old), but it has more expressive pod selectors A DaemonSet is mostly for specific case like infrastructure-related pods that perform system-level operations in which a pod must run on each and every node in the cluster and each node needs to run exactly one instance of the pod, for example log collector and resource monitor A Job resource allows to run a pod whose container isn't restarted when the process running inside finishes successfully. A cron job in Kubernetes is configured by creating a CronJob resource A Service represents a static location for a group of one or more pods that all provide the same service. Requests coming to the IP and port of the service will be forwarded/load-balanced to the IP and port of one of the pods belonging to the service at that moment ExternalName type, a service that serves as an alias for an external service NodePort type, each cluster node opens a port on the node itself and redirects traffic received on that port to the underlying service LoadBalancer type (extension of NodePort), makes the service accessible through a dedicated load balancer which usually is supported and automatically provisioned by the cloud infrastructure A headless service still provides load balancing across pods, but through the DNS round-robin mechanism instead of through the service proxy, because DNS returns the pods' IPs, clients connect directly to the pods, instead of through the service proxy. Setting the clusterIP field in a service spec to None makes the service headless An Endpoints resource is a list of IP addresses and ports exposing a service. The Endpoints object needs to have the same name as the service and contain the list of target IP addresses and ports for the service An Ingress operates at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity. LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one The Kubelet on the node hosting the pod can check if a container is still alive through liveness probes using httpGet , tcpSocket or exec . Exit code is a sum of 128 + N e.g. 137 = 128 + 9 (SIGKILL) or 143 = 128 + 15 (SIGTERM). Always remember to set an initial delay initialDelaySeconds The readiness probes is invoked periodically and determines whether the specific pod should receive client requests or not. Liveness probes keep pods healthy by killing off unhealthy containers and replacing them with new, healthy ones, whereas readiness probes make sure that only pods that are ready to serve requests receive them and this is mostly necessary during container start up A volume is a component of a pod and not a standalone object, it cannot be created or deleted on its own. A volume is available to all containers in the pod, but it must be mounted in each container that needs to access it emptyDir , a simple empty directory used for storing transient data hostPath , used for mounting directories from the worker node's filesystem into the pod gitRepo , a volume initialized by checking out the contents of a Git repository nfs , an NFS share mounted into the pod gcePersistentDisk (Google Compute Engine Persistent Disk), awsElasticBlockStore (Amazon Web Services Elastic Block Store Volume), azureDisk (Microsoft Azure Disk Volume), used for mounting cloud provider-specific storage cinder, cephfs, iscsi, flocker, glusterfs, quobyte, rbd, flexVolume, vsphereVolume, photonPersistentDisk, scaleIO , used for mounting other types of network storage configMap, secret, downwardAPI , a special types of volumes used to expose certain Kubernetes resources and cluster information to the pod persistentVolumeClaim , a way to use a pre- or dynamically provisioned persistent storage An app can be configured by passing command-line arguments to containers with command and args setting custom environment variables for each container of a pod mounting configuration files into containers through a special type of volume The contents of a ConfigMap are passed to containers as either environment variables or as files in a volume while on the nodes Secrets are always stored in memory and never written to physical storage (maximum size of a Secret is limited to 1MB) The Downward API enables you to expose the pod's own metadata to the processes running inside that pod (it isn't a REST endpoint) Rolling update means replace pods step by step slowly scaling down the previous version and scaling up the new one A Deployment is a higher-level resource meant for deploying applications and updating them declaratively, instead of doing it through a ReplicationController or a ReplicaSet, which are both considered lower-level concepts. A Deployment doesn't manage pods directly, instead it creates a new ReplicaSet which is scaled up slowly, while the previous ReplicaSet is scaled down to zero. See also minReadySeconds , maxSurge and maxUnavailable properties A StatefulSet makes sure pods are rescheduled in such a way that they retain their identity and state. StatefulSets were initially called PetSets, that name comes from the pets vs. cattle analogy . Each pod created by a StatefulSet is assigned an ordinal index (zero-based), which is then used to derive the pod's name and hostname, and to attach stable storage to the pod. A StatefulSet requires to create a corresponding governing headless Service ( clusterIP=None ) that's used to provide the actual network identity to each pod, in this case each pod gets its own DNS entry. The new pod isn't necessarily scheduled to the same node. Scaling the StatefulSet creates a new pod instance with the next unused ordinal index. Scaling down a StatefulSet always removes the instances with the highest ordinal index first. StatefulSets don't delete PersistentVolumeClaims when scaling down and they reattach them when scaling back up Clients watch for changes by opening an HTTP connection to the API server. Through this connection, the client will then receive a stream of modifications to the watched objects The Scheduler waits for newly created pods through the API server's watch mechanism and assign a node to each new pod that doesn't already have the node set. The Scheduler doesn't instruct the selected node (or the Kubelet running on that node) to run the pod. All the Scheduler does is update the pod definition through the API server. The API server then notifies the Kubelet that the pod has been scheduled. As soon as the Kubelet on the target node sees the pod has been scheduled to its node, it creates and runs the pod's containers The single Controller Manager process currently combines a multitude of controllers performing various reconciliation tasks Replication Manager (a controller for ReplicationController resources) ReplicaSet, DaemonSet, and Job controllers Deployment controller StatefulSet controller Node controller Service controller Endpoints controller Namespace controller PersistentVolume controller Others Controllers do many different things, but they all watch the API server for changes to resources and perform operations for each change. In general, controllers run a reconciliation loop, which reconciles the actual state with the desired state (specified in the resource's spec section) and writes the new actual state to the resource's status section The Kubelet is the component responsible for everything running on a worker node. Its initial job is to register the node it's running on by creating a Node resource in the API server. Then it needs to continuously monitor the API server for Pods that have been scheduled to the node, and start the pod's containers. It does this by telling the configured container runtime (i.e. Docker) to run a container from a specific container image. The Kubelet then constantly monitors running containers and reports their status, events, and resource consumption to the API server. The Kubelet is also the component that runs the container liveness probes, restarting containers when the probes fail. Lastly, it terminates containers when their Pod is deleted from the API server and notifies the server that the pod has terminated Every worker node also runs the kube-proxy (Service Proxy), whose purpose is to make sure clients can connect to the services you define through the Kubernetes API and performs load balancing across those pods. The current, implementation only uses iptables rules to redirect packets to a randomly selected backend pod without passing them through an actual proxy server Every pod is associated with a ServiceAccount , which represents the identity of the app running in the pod. The token file /var/run/secrets/kubernetes.io/serviceaccount/token , which is mounted into each container's filesystem through a secret volume, holds the ServiceAccount's authentication token used to connect to the API server RBAC (role-based access control) plugin prevents unauthorized users from viewing or modifying the cluster state. Roles are managed by Roles , ClusterRoles , RoleBindings and ClusterRoleBindings resources. Cluster level resources are not namespaced. Roles define what can be done, while bindings define who can do it","title":"Architecture"},{"location":"kubernetes/#setup","text":"Requirements Minikube VirtualBox Local cluster # verify installation minikube version # lifecycle minikube start --vm-driver=virtualbox minikube stop minikube delete # dashboard export NO_PROXY=localhost,127.0.0.1,$(minikube ip) minikube dashboard # access minikube ssh docker ps -a # reuse the minikube's built-in docker daemon eval $(minikube docker-env) # access NodePort services minikube service <SERVICE_NAME> [-n <NAMESPACE>] # list addons minikube addons list # enable addon minikube addons enable <ADDON_NAME> # (?) swagger minikube start --extra-config=apiserver.Features.EnableSwaggerUI=true kubectl proxy open http://localhost:8080/swagger-ui Basic # verify installation kubectl version # cluster info kubectl cluster-info kubectl get nodes kubectl describe nodes kubectl config view # health status of each control plane component kubectl get componentstatuses # namespace kubectl create namespace <NAMESPACE_NAME> kubectl get namespaces kubectl config view | grep namespace kubectl delete namespace <NAMESPACE_NAME> # current namespace kubectl config current-context # switch namespace kubectl config set-context $(kubectl config current-context) --namespace=<NAMESPACE_NAME> # create/update resources from file kubectl create -f <FILE_NAME>.yaml kubectl apply -f <FILE_NAME>.yaml # explain fields kubectl explain pod kubectl explain service.spec # edit resource (vim) kubectl edit pod <POD_NAME> # verbosity kubectl get pods --v 6 # access etcd on minikube minikube ssh docker run \\ --rm \\ -p 12379:2379 \\ -p 12380:2380 \\ --mount type=bind,source=/tmp/etcd-data.tmp,destination=/etcd-data \\ --name etcd-gcr-v3.3.10 \\ gcr.io/etcd-development/etcd:v3.3.10 \\ /usr/local/bin/etcd \\ --name s1 \\ --data-dir /etcd-data \\ --listen-client-urls http://0.0.0.0:12379 \\ --advertise-client-urls http://0.0.0.0:12379 \\ --listen-peer-urls http://0.0.0.0:12380 \\ --initial-advertise-peer-urls http://0.0.0.0:12380 \\ --initial-cluster s1=http://0.0.0.0:12380 \\ --initial-cluster-token tkn \\ --initial-cluster-state new docker exec etcd-gcr-v3.3.10 /bin/sh -c \"ETCDCTL_API=3 /usr/local/bin/etcdctl version\" docker exec etcd-gcr-v3.3.10 /bin/sh -c \"ETCDCTL_API=3 /usr/local/bin/etcdctl get /registry --endpoints=127.0.0.1:12379\" # watch cluster events kubectl get events --watch Simple deployment # deploy demo app kubectl run kubernetes-bootcamp \\ --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\ --port=8080 \\ --labels='app=kubernetes-bootcamp' # update app kubectl set image deployments/kubernetes-bootcamp \\ kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2 # verify update kubectl rollout status deployments/kubernetes-bootcamp kubectl rollout history deployments/kubernetes-bootcamp # undo latest deployment kubectl rollout undo deployments/kubernetes-bootcamp # undo deployment to revision N kubectl rollout undo deployment <DEPLOYMENT_NAME> --to-revision=N # pause/resume deployment kubectl rollout pause deployment <DEPLOYMENT_NAME> kubectl rollout resume deployment <DEPLOYMENT_NAME> # list deployments kubectl get deployments kubectl describe deployment # create deployment from file (records the command in the revision history) kubectl create -f <FILE_NAME>.yaml --record # manually slowdown a rolling update kubectl patch deployment <DEPLOYMENT_NAME> -p '{\"spec\": {\"minReadySeconds\": 10}}' Pod and Container # proxy cluster (open in 2nd terminal) kubectl proxy # pod name export POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo POD_NAME=$POD_NAME # verify proxy http :8001/version http :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/ # view logs kubectl logs $POD_NAME kubectl logs <POD_NAME> -c <CONTAINER_NAME> kubectl logs <POD_NAME> --previous # execute command on container kubectl exec $POD_NAME printenv kubectl exec $POD_NAME ls -- -la # access container kubectl exec -it $POD_NAME bash # verify label kubectl describe pods $POD_NAME # list containers inside pods kubectl describe pods # list pods kubectl get pods # list pods and nodes kubectl get pods -o wide # list pods with labels kubectl get po --show-labels kubectl get pods -L <LABEL_KEY> # filter with equality-based labels kubectl get pods -l app=kubernetes-bootcamp kubectl get pods -l app kubectl get pods -l '!app' # filter with set-based labels kubectl get pods -l 'app in (kubernetes-bootcamp)' # add/update labels manually kubectl label po <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> kubectl label po <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> --overwrite # annotate kubectl annotate pod <POD_NAME> <LABEL_KEY>=<LABEL_VALUE> # print definition kubectl get pod <POD_NAME> -o json kubectl get pod <POD_NAME> -o yaml # output json kubectl get pod <POD_NAME> -o json | jq '.metadata' # output json kubectl get pod <POD_NAME> -o yaml | yq '.metadata.annotations' # output yaml kubectl get pod <POD_NAME> -o yaml | yq -y '.metadata.annotations' # by namespace kubectl get ns kubectl get pod --namespace kube-system kubectl get po --all-namespaces # watch for changes kubectl get pods --watch kubectl get pods -o yaml --watch # (debug) forward a local network port to a port in the pod (without service) kubectl port-forward <POD_NAME> <LOCAL_PORT>:<POD_PORT> # delete (sends SIGTERM to containers and waits 30 seconds, otherwise sends SIGKILL) kubectl delete po <POD_NAME> kubectl delete pod -l <LABEL_KEY>=<LABEL_VALUE> kubectl delete po --all Service # list services kubectl get svc kubectl get services # create service kubectl expose deployment/kubernetes-bootcamp \\ --type=\"NodePort\" \\ --port 8080 # service info kubectl describe services/kubernetes-bootcamp # expose service export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT # verify service curl $(minikube ip):$NODE_PORT # add 4 replicas kubectl scale deployments/kubernetes-bootcamp --replicas=4 # info kubectl get pod,deployment,service kubectl get pods -o wide kubectl describe deployments/kubernetes-bootcamp # cleanup kubectl delete deployment,service kubernetes-bootcamp # all (means all resource types) # --all (means all resource instances) kubectl delete all --all Other # replication controller kubectl get rc kubectl get replicationcontroller # replica set kubectl get rs kubectl get replicaset # jobs kubectl get jobs # ingress kubectl get ingress # persistent volume, claim and storage class kubectl get pv kubectl get pvc kubectl get sc # jsonpath example kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"ExternalIP\")].address}' # create config map kubectl create configmap my-config \\ --from-literal=my-key-1=my-value-1 \\ --from-literal=my-key-2=my-value-2 # print config-map kubectl get configmap my-config -o yaml # secrets kubectl get secrets kubectl describe secrets kubectl exec <POD_NAME> ls /var/run/secrets/kubernetes.io/serviceaccount/ # create secrets (generic|tls|docker-registry) echo bar > foo.secure kubectl create secret generic my-secret --from-file=foo.secure # contents are shown as Base64-encoded strings kubectl get secret my-secret -o yaml # prints \"bar\" echo YmFyCg== | base64 -D # rbac kubectl get clusterroles kubectl get clusterrolebindings kubectl get roles kubectl get rolebindings # access api server from local machine kubectl proxy http :8001 http :8001/api/v1/pods # access api server from a container # (create) kubectl run hello-curl \\ --image=tutum/curl \\ --command -- \"sleep\" \"9999999\" # (exec) kubectl exec -it hello-curl-XXX bash # (verify envs) printenv # (missing certificate) curl -k https://kubernetes # (list secrets) ls -laht /var/run/secrets/kubernetes.io/serviceaccount/ # (specify certificate) curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt https://kubernetes # (specify token, namespace and default certificate) or \"https://kubernetes.default/api/v1\" export KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) export KUBE_NS=$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace) export CURL_CA_BUNDLE=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt curl -v \\ -H \"Authorization: Bearer $KUBE_TOKEN\" \\ https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/$KUBE_NS/pods # @see also ambassador container pattern based on kubectl-proxy # api server proxy to invoke endpoint on resource kubectl proxy curl localhost:8001/api/v1/namespaces/<NAMESPACE_NAME>/<pods|services|...>/<RESOURCE_NAME>/proxy/<PATH> # temporary command kubectl run \\ -it srvlookup \\ --image=tutum/dnsutils \\ --rm \\ --restart=Never \\ -- dig <SERVICE_NAME> kubia.default.svc.cluster.local # temporary container kubectl run \\ -it phusion \\ --image=phusion/baseimage:latest \\ --rm \\ --restart=Never \\ bash Alternative ways of modify resources # opens the object's manifest in default editor kubectl edit # modifies individual properties of an object kubectl patch # creates or modifies the object by applying property values from a full YAML or JSON file kubectl apply # same as apply but only if not exists kubectl create # same as apply but only if exists kubectl replace # changes the container image kubectl set image","title":"Setup"},{"location":"kubernetes/#helm","text":"Helm installs charts into Kubernetes, creating a new release for each installation. And to find new charts, you can search Helm chart repositories A Chart is a Helm package. It contains all of the resource definitions necessary to run an application, tool, or service inside of a Kubernetes cluster. Think of it like the Kubernetes equivalent of a Homebrew formula, an Apt dpkg, or a Yum RPM file A Repository is the place where charts can be collected and shared A Release is an instance of a chart running in a Kubernetes cluster Resources Documentation The missing CI/CD Kubernetes component: Helm package manager Commands # linux sudo snap install helm --classic # mac brew install kubernetes-helm # setup client only helm init --client-only # search helm search <CHART> helm search postgresql # info helm inspect <CHART> helm inspect stable/postgresql helm inspect values stable/postgresql helm get values <KEY> helm status <CHART> helm list helm list --all # install public chart [install|upgrade|rollback|delete] helm install <CHART> helm install -f <CONFIG_OVERRIDE>.yaml <CHART> helm delete <CHART> # repository helm repo list helm repo add <NAME> https://<DOMAIN>/<PATH> helm repo update # custom chart helm create my-chart helm lint my-chart helm package my-chart helm install ./my-chart helm install --dry-run --debug ./my-chart # plugin $(helm home)/plugins helm plugin install <PATH|URL>","title":"Helm"},{"location":"linux/","text":"Linux Resources How Linux Works (2014)(2nd) by Brian Ward (Book) Attacking Network Protocols (2018) by James Forshaw (Book) Practical Packet Analysis (2017)(3rd) by Chris Sanders (Book) The TCP/IP Guide (2005) by Charles M. Kozierok Kernel documentation Interactive map of Linux kernel Linux Performance Useful commands # create nested directories mkdir -p parent/child1/child2 && cd $_ # scroll file from bottom less +G /var/log/auth.log # follow also if doesn't exist tail -F /var/log/auth.log # find files find /etc -name '*shadow' # prints lines that match regexp # -i case insensitive # -v inverts the search # -c count lines grep -E '^root' /etc/passwd # password encryption grep password.*unix /etc/pam.d/* # sed = stream editor # example substitution echo -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt cat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq # delete lines three through six sed 3,6d /etc/passwd # pick a single field out of an input stream ls -l | awk '{print $9}' # extract 2nd string echo \"aaa bbb ccc\" > test.txt cat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}' # pack archive tar cvf archive.tar file1 file2 # table-of-content tar tvf archive.tar # unpack archive tar xvf archive.tar -C output # compress and pack archive tar zcvf archive.tar.gz /path/to/images/*.jpg # unpack compressed archive tar zxvf archive.tar.gz # pack archive zip -r backup.zip file-name directory-name # zip with password (prompt) zip -e backup.zip file-name # unpack jar unzip my-lib.jar -d /tmp/my-lib # count lines wc -l file # lowercase random uuid uuidgen | tr \"[:upper:]\" \"[:lower:]\" # number of bytes stat --printf=\"%s\" file # calculator echo 1+2 | bc # print number in binary base 2 format echo 'obase=2; 240' | bc # reverse-polish calculator echo '1 2 + p' | dc # evaluate expressions expr 1 + 2 # unix timestamp date +%s # timestamp in microsecond date +%s%N # calendar cal -3 # configure kernel parameters at runtime sysctl # test conditions ([) test a = a && echo equal # create temporary file mktemp # X is a template mktemp /tmp/my-tmp.XXXXXX # signal handler to catch the signal that CTRL-C generates and remove the temporary files TMPFILE=$(mktemp /tmp/my-tmp.XXXXXX) trap \"rm -f $TMPFILE; exit 1\" INT # compare files diff FILE1 FILE2 # here document DATE=$(date) cat <<EOF Date: $DATE line1 line2 EOF # strip full path and extension if specified e.g. mail basename /var/log/mail.log .log # image conversion giftopnm pnmtopng # when operating on huge number of files to avoid buffer issues # e.g. verify file's type # INSECURE find . -name '*.md' -print | xargs file # change the find output separator and the xargs argument delimiter from a newline to a NULL character # two dashes if there is a chance that any of the target files start with a single dash find . -name '*.md' -print0 | xargs -0 file -- # supply a {} to substitute the filename and a literal ; to indicate the end of the command find . -name '*.md' -exec file {} \\; # replaces current shell process with the program you name after exec system call # after you press CTRL-D or CTRL-C to terminate the cat program, # your window should disappear because its child process no longer exists exec cat # subshell example () e.g. path remains the same outside (PATH=/bad/invalid:$PATH; echo $PATH) # fast way to copy and preserve permissions tar cf - orig | (cd target; tar xvf -) # X Window System xwininfo xlsclients -l xev xinput --list dbus-monitor --system dbus-monitor --session # compile C program cc -o hello hello.c # list shared library (so) ldd /bin/bash # create hex dump xxd /usr/bin/bc # open binary vim /usr/bin/bc # (start) edit hex dump :%!xxd # (finish) edit hex dump :%!xxd -r # disassemble binary objdump -d /usr/bin/bc Script templates # shebang #!/bin/sh #!/bin/bash # unofficial bash strict mode set -euo pipefail IFS=$'\\n\\t' # run from any directory (no symlink allowed) CURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P) cd ${CURRENT_PATH} # import . imported_file.sh source imported_file.sh # read and store in a variable read MY_VAR echo $MY_VAR # read stdin read -p \"Are you sure? [y/n]\" -n 1 -r Diagnostic # sysfs info udevadm info --query=all --name=/dev/xvda # monitor kernel uevents udevadm monitor # view kernel's boot and runtime diagnostic messages dmesg | less # system logs paths configuration vim /etc/rsyslog.conf vim /etc/rsyslog.d/50-default.conf # test system logger logger -p mail.info mail-message tail -n 1 /var/log/syslog Filesystem # copy data in blocks of a fixed size # /dev/zero is a continuous stream of zero bytes dd if=/dev/zero of=DUMP_FILE bs=1024 count=1 # view partition table # use (g)parted only for partioning disk (supports MBR and GPT) sudo parted -l # create filesystem mkfs -t ext4 /dev/PARTITION_NAME ls -l /sbin/mkfs.* # list devices and corresponding filesystems UUID blkid # list attached filesystems mount # mount device on mount point mount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT # mount filesystem by its UUID mount UUID=xxx-yyy-zzz /MOUNT/POINT # make changes permanent after reboot echo \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME) /MOUNT/POINT ext4 defaults,nofail 0 2\" | sudo tee -a /etc/fstab # mount all filesystems mount -a # unmount (detach) a filesystem umount /dev/PARTITION_NAME # view size and utilization of mounted filesystems df -h # disk usage du -sh /* | sort -g # disk size fdisk --list # check memory and swap size free -h # (1) create swap file (~1GB) dd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000 # (2) create swap file (2GB) fallocate -l 2G /dev/SWAP_FILE # change owner and permissions chown root:root /dev/SWAP_FILE chmod 0600 /dev/SWAP_FILE # put swap signature on partition mkswap /dev/SWAP_FILE # register space with the kernel swapon /dev/SWAP_FILE # make changes permanent after reboot echo \"/dev/SWAP_FILE none swap sw 0 0\" | tee -a /etc/fstab # list swap partitions swapon --show # simple static server on port 8000 python -m SimpleHTTPServer # copy directory to remote host scp -r directory remote_host:~/new-directory tar cBvf - directory | ssh remote_host tar xBvpf - rsync -az directory remote_host:~/new- # equivalent to /* # -nv dry run rsync -a directory/ remote_host:~/new-directory Monitoring # list processes # m show threads ps aux # display current system status # Spacebar Updates the display immediately # M Sorts by current resident memory usage # T Sorts by total (cumulative) CPU usage # P Sorts by current CPU usage (the default) # u Displays only one user\u2019s processes # f Selects different statistics to display # ? Displays a usage summary for all top commands top top -p PID1 PID2 # alternatives htop atop # monitor system performance vmstat 2 # list open files and the processes using them lsof | less lsof /dev # print all the system calls that a process makes strace cat /dev/null strace uptime strace -e trace=network,read,write bc # track shared library calls ltrace ls / # allows to set system-wide probes on special trace providers e.g. system calls dtrace # CPU usage /usr/bin/time ls # change process priority (-20 < nice value < +20) renice 20 PID # load average: for the past 1 minute, 5 minutes and 15 minutes uptime # check memory status free cat /proc/meminfo # check major/minor page faults /usr/bin/time cal > /dev/null # show statistics for machine\u2019s current uptime (install sysstat) iostat # show partition information iostat -p ALL # show I/O resources used by individual processes iotop # see the resource consumption of a process over time pidstat -p PID 1 # reports CPU and IO stats iostat -mt 2 # system resource statistics dstat Resources Dynamic Tracing with DTrace & SystemTap Network # active network interfaces ifconfig # enable/disable network interface ifconfig NETWORK_INTERFACE up ifconfig NETWORK_INTERFACE down # show routing table # Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route) # flag U: up # flag G: gateway # convention: the router is usually at address 1 of the subnet route -n # ICMP echo request # icmp_req: verify order and no gap # time: round-trip time ping -c 3 8.8.8.8 # show path packets take to a remote host traceroute 8.8.8.8 # (DNS) find the IP address behind a domain name host www.github.com # network manager nmcli nmcli device show # returns zero as its exit code if network is up nm-online # network details e.g. ssid/password cat /etc/NetworkManager/system-connections/NETWORK_NAME # override hostname lookups vim /etc/hosts # traditional configuration file for DNS servers cat /etc/resolv.conf # DNS settings cat /etc/nsswitch.conf # static IP /etc/network/interfaces # list of assigned port numbers /etc/services # -t Prints TCP port information # -u Prints UDP port information # -l Prints listening ports # -a Prints every active port # -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working) # list open TCP connections netstat -nt # print listening TCP ports netstat -ntl # list running services netstat -plunt # processes listening on open TCP ports lsof -i -n -P | grep TCP lsof -iTCP -sTCP:LISTEN # process running on specific port lsof -n -i:PORT_NUMBER # list unix domain socket lsof -U # well-known ports cat /etc/services # release IP with DHCP dhclient -r NETWORK_INTERFACE_NAME # renew IP dhclient -v NETWORK_INTERFACE_NAME # public IP via external services http ident.me http ipv4.ident.me http ipv6.ident.me http icanhazip.com http ipv4.icanhazip.com http ipv6.icanhazip.com # Linux kernel does not automatically move packets from one subnet to another # enable temporary IP forwarding in the router's kernel sysctl net.ipv4.conf.all.forwarding=1 sysctl net.ipv6.conf.all.forwarding=1 # on macOS sysctl -w net.ipv4.ip_forward # change permanent configs upon reboot vim /etc/sysctl.conf # example NAT (IP masquerading) sysctl -w net.ipv4.ip_forward iptables -P FORWARD DROP iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth1 -o eth0 -j ACCEPT # firewalling on individual machines is sometimes called IP filtering # firewall rules in series or chain make up a table # INPUT chain: protect individual machine # FORWARD chain: protect a network of machines # show iptable configuration iptables -L # block IP iptables -A INPUT -s BLOCKED_IP -j DROP # block IP/port iptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP # allow IP (insert at the bottom) iptables -A INPUT -s ALLOWED_IP -j ACCEPT # allow IP (insert at the top) iptables -I INPUT -s ALLOWED_IP -j ACCEPT # allow IP (specify order) iptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT # delete rule #number in chain iptables -D INPUT RULE_NUMBER # flush any existing NAT rules iptables -t nat -F # source NAT (SNAT) or masquerading: changes the IP source address information iptables -t nat -A POSTROUTING -o INTNAME -j SNAT --to INTIP iptables -t nat -A POSTROUTING -o INTNAME -j MASQUERADE # destination NAT (DNAT): changes the destination address iptables -t nat -A PREROUTING -d ORIGIP -j DNAT --to-destination NEWIP iptables -t nat -A PREROUTING -p PROTO -d ORIGIP --dport ORIGPORT -j DNAT --to-destination NEWIP:NEWPORT # show ARP kernel cache arp -n # list wireless network iw dev NETWORK_INTERFACE scan # show details of connected network iw dev NETWORK_INTERFACE link # manage both authentication and encryption for a wireless network interface wpa_supplicant Active network traffic capture Port-Forwarding proxy SOCKS proxy (preferred) # specify a SOCKS proxy for any TCP connection java \u2013DsocksProxyHost=localhost \u2013DsocksProxyPort=1080 XXX HTTP forwarding proxy (client side) HTTP reverse proxy (server side) Resources Subnetting dsniff Netfilter Dante - A free SOCKS server Secure Socket Funneling Iptables Essentials iptables vs nftables Applications # old insecure telnet www.wikipedia.org 80 # press enter twice after GET / HTTP/1.0 # details about communication curl --trace-ascii trace_file https://www.wikipedia.org > /dev/null vim trace_file # sshd server configs vim /etc/ssh/sshd_config # generate key pair ssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH # list network interfaces tcpdump -D # sniff hex and ascii (-A) by interface/host/port tcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER # Swiss Army knife # banner grabbing cat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER # install traditional (with -e option) apt-get install netcat -y # choose /bin/nc.traditional update-alternatives --config nc # listen on server netcat -l -p 6996 -e /bin/bash # run client cat <(echo ls -la) - | netcat IP_ADDRESS 6996 # simplest fuzz test cat /dev/urandom | nc IP_ADDRESS PORT_NUMBER # scan open ports nmap -Pn IP_ADDRESS # other # * ssh/scp/sftp/rsync # * curl/wget # system calls man recv man send # DHCP is designed to run on IP networks to distribute network configuration # information to nodes automatically: # IP address, default gateway, routing tables, default DNS servers, etc... # ARP finds the Ethernet address for a given IP address # * DHCP Spoofing # * ARP Poisoning ettercap -G # cli debuggers # CDB on Windows, GDB on Linux, and LLDB on macOS Resources Wireshark tcpdump tshark Nmap Ettercap Service Name and Transport Protocol Port Number Registry TODO add more examples Shell/Terminal Fish Oil Zellij Blogs Highly useful Linux commands & configurations Other resources OpenWrt BusyBox X.509 RFC OpenSSL Command-Line HOWTO Program Library HOWTO Shorewall Postfix HTTPie jq Lynx Samba","title":"Linux"},{"location":"linux/#linux","text":"Resources How Linux Works (2014)(2nd) by Brian Ward (Book) Attacking Network Protocols (2018) by James Forshaw (Book) Practical Packet Analysis (2017)(3rd) by Chris Sanders (Book) The TCP/IP Guide (2005) by Charles M. Kozierok Kernel documentation Interactive map of Linux kernel Linux Performance","title":"Linux"},{"location":"linux/#useful-commands","text":"# create nested directories mkdir -p parent/child1/child2 && cd $_ # scroll file from bottom less +G /var/log/auth.log # follow also if doesn't exist tail -F /var/log/auth.log # find files find /etc -name '*shadow' # prints lines that match regexp # -i case insensitive # -v inverts the search # -c count lines grep -E '^root' /etc/passwd # password encryption grep password.*unix /etc/pam.d/* # sed = stream editor # example substitution echo -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt cat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq # delete lines three through six sed 3,6d /etc/passwd # pick a single field out of an input stream ls -l | awk '{print $9}' # extract 2nd string echo \"aaa bbb ccc\" > test.txt cat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}' # pack archive tar cvf archive.tar file1 file2 # table-of-content tar tvf archive.tar # unpack archive tar xvf archive.tar -C output # compress and pack archive tar zcvf archive.tar.gz /path/to/images/*.jpg # unpack compressed archive tar zxvf archive.tar.gz # pack archive zip -r backup.zip file-name directory-name # zip with password (prompt) zip -e backup.zip file-name # unpack jar unzip my-lib.jar -d /tmp/my-lib # count lines wc -l file # lowercase random uuid uuidgen | tr \"[:upper:]\" \"[:lower:]\" # number of bytes stat --printf=\"%s\" file # calculator echo 1+2 | bc # print number in binary base 2 format echo 'obase=2; 240' | bc # reverse-polish calculator echo '1 2 + p' | dc # evaluate expressions expr 1 + 2 # unix timestamp date +%s # timestamp in microsecond date +%s%N # calendar cal -3 # configure kernel parameters at runtime sysctl # test conditions ([) test a = a && echo equal # create temporary file mktemp # X is a template mktemp /tmp/my-tmp.XXXXXX # signal handler to catch the signal that CTRL-C generates and remove the temporary files TMPFILE=$(mktemp /tmp/my-tmp.XXXXXX) trap \"rm -f $TMPFILE; exit 1\" INT # compare files diff FILE1 FILE2 # here document DATE=$(date) cat <<EOF Date: $DATE line1 line2 EOF # strip full path and extension if specified e.g. mail basename /var/log/mail.log .log # image conversion giftopnm pnmtopng # when operating on huge number of files to avoid buffer issues # e.g. verify file's type # INSECURE find . -name '*.md' -print | xargs file # change the find output separator and the xargs argument delimiter from a newline to a NULL character # two dashes if there is a chance that any of the target files start with a single dash find . -name '*.md' -print0 | xargs -0 file -- # supply a {} to substitute the filename and a literal ; to indicate the end of the command find . -name '*.md' -exec file {} \\; # replaces current shell process with the program you name after exec system call # after you press CTRL-D or CTRL-C to terminate the cat program, # your window should disappear because its child process no longer exists exec cat # subshell example () e.g. path remains the same outside (PATH=/bad/invalid:$PATH; echo $PATH) # fast way to copy and preserve permissions tar cf - orig | (cd target; tar xvf -) # X Window System xwininfo xlsclients -l xev xinput --list dbus-monitor --system dbus-monitor --session # compile C program cc -o hello hello.c # list shared library (so) ldd /bin/bash # create hex dump xxd /usr/bin/bc # open binary vim /usr/bin/bc # (start) edit hex dump :%!xxd # (finish) edit hex dump :%!xxd -r # disassemble binary objdump -d /usr/bin/bc Script templates # shebang #!/bin/sh #!/bin/bash # unofficial bash strict mode set -euo pipefail IFS=$'\\n\\t' # run from any directory (no symlink allowed) CURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P) cd ${CURRENT_PATH} # import . imported_file.sh source imported_file.sh # read and store in a variable read MY_VAR echo $MY_VAR # read stdin read -p \"Are you sure? [y/n]\" -n 1 -r","title":"Useful commands"},{"location":"linux/#diagnostic","text":"# sysfs info udevadm info --query=all --name=/dev/xvda # monitor kernel uevents udevadm monitor # view kernel's boot and runtime diagnostic messages dmesg | less # system logs paths configuration vim /etc/rsyslog.conf vim /etc/rsyslog.d/50-default.conf # test system logger logger -p mail.info mail-message tail -n 1 /var/log/syslog","title":"Diagnostic"},{"location":"linux/#filesystem","text":"# copy data in blocks of a fixed size # /dev/zero is a continuous stream of zero bytes dd if=/dev/zero of=DUMP_FILE bs=1024 count=1 # view partition table # use (g)parted only for partioning disk (supports MBR and GPT) sudo parted -l # create filesystem mkfs -t ext4 /dev/PARTITION_NAME ls -l /sbin/mkfs.* # list devices and corresponding filesystems UUID blkid # list attached filesystems mount # mount device on mount point mount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT # mount filesystem by its UUID mount UUID=xxx-yyy-zzz /MOUNT/POINT # make changes permanent after reboot echo \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME) /MOUNT/POINT ext4 defaults,nofail 0 2\" | sudo tee -a /etc/fstab # mount all filesystems mount -a # unmount (detach) a filesystem umount /dev/PARTITION_NAME # view size and utilization of mounted filesystems df -h # disk usage du -sh /* | sort -g # disk size fdisk --list # check memory and swap size free -h # (1) create swap file (~1GB) dd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000 # (2) create swap file (2GB) fallocate -l 2G /dev/SWAP_FILE # change owner and permissions chown root:root /dev/SWAP_FILE chmod 0600 /dev/SWAP_FILE # put swap signature on partition mkswap /dev/SWAP_FILE # register space with the kernel swapon /dev/SWAP_FILE # make changes permanent after reboot echo \"/dev/SWAP_FILE none swap sw 0 0\" | tee -a /etc/fstab # list swap partitions swapon --show # simple static server on port 8000 python -m SimpleHTTPServer # copy directory to remote host scp -r directory remote_host:~/new-directory tar cBvf - directory | ssh remote_host tar xBvpf - rsync -az directory remote_host:~/new- # equivalent to /* # -nv dry run rsync -a directory/ remote_host:~/new-directory","title":"Filesystem"},{"location":"linux/#monitoring","text":"# list processes # m show threads ps aux # display current system status # Spacebar Updates the display immediately # M Sorts by current resident memory usage # T Sorts by total (cumulative) CPU usage # P Sorts by current CPU usage (the default) # u Displays only one user\u2019s processes # f Selects different statistics to display # ? Displays a usage summary for all top commands top top -p PID1 PID2 # alternatives htop atop # monitor system performance vmstat 2 # list open files and the processes using them lsof | less lsof /dev # print all the system calls that a process makes strace cat /dev/null strace uptime strace -e trace=network,read,write bc # track shared library calls ltrace ls / # allows to set system-wide probes on special trace providers e.g. system calls dtrace # CPU usage /usr/bin/time ls # change process priority (-20 < nice value < +20) renice 20 PID # load average: for the past 1 minute, 5 minutes and 15 minutes uptime # check memory status free cat /proc/meminfo # check major/minor page faults /usr/bin/time cal > /dev/null # show statistics for machine\u2019s current uptime (install sysstat) iostat # show partition information iostat -p ALL # show I/O resources used by individual processes iotop # see the resource consumption of a process over time pidstat -p PID 1 # reports CPU and IO stats iostat -mt 2 # system resource statistics dstat Resources Dynamic Tracing with DTrace & SystemTap","title":"Monitoring"},{"location":"linux/#network","text":"# active network interfaces ifconfig # enable/disable network interface ifconfig NETWORK_INTERFACE up ifconfig NETWORK_INTERFACE down # show routing table # Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route) # flag U: up # flag G: gateway # convention: the router is usually at address 1 of the subnet route -n # ICMP echo request # icmp_req: verify order and no gap # time: round-trip time ping -c 3 8.8.8.8 # show path packets take to a remote host traceroute 8.8.8.8 # (DNS) find the IP address behind a domain name host www.github.com # network manager nmcli nmcli device show # returns zero as its exit code if network is up nm-online # network details e.g. ssid/password cat /etc/NetworkManager/system-connections/NETWORK_NAME # override hostname lookups vim /etc/hosts # traditional configuration file for DNS servers cat /etc/resolv.conf # DNS settings cat /etc/nsswitch.conf # static IP /etc/network/interfaces # list of assigned port numbers /etc/services # -t Prints TCP port information # -u Prints UDP port information # -l Prints listening ports # -a Prints every active port # -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working) # list open TCP connections netstat -nt # print listening TCP ports netstat -ntl # list running services netstat -plunt # processes listening on open TCP ports lsof -i -n -P | grep TCP lsof -iTCP -sTCP:LISTEN # process running on specific port lsof -n -i:PORT_NUMBER # list unix domain socket lsof -U # well-known ports cat /etc/services # release IP with DHCP dhclient -r NETWORK_INTERFACE_NAME # renew IP dhclient -v NETWORK_INTERFACE_NAME # public IP via external services http ident.me http ipv4.ident.me http ipv6.ident.me http icanhazip.com http ipv4.icanhazip.com http ipv6.icanhazip.com # Linux kernel does not automatically move packets from one subnet to another # enable temporary IP forwarding in the router's kernel sysctl net.ipv4.conf.all.forwarding=1 sysctl net.ipv6.conf.all.forwarding=1 # on macOS sysctl -w net.ipv4.ip_forward # change permanent configs upon reboot vim /etc/sysctl.conf # example NAT (IP masquerading) sysctl -w net.ipv4.ip_forward iptables -P FORWARD DROP iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE iptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth1 -o eth0 -j ACCEPT # firewalling on individual machines is sometimes called IP filtering # firewall rules in series or chain make up a table # INPUT chain: protect individual machine # FORWARD chain: protect a network of machines # show iptable configuration iptables -L # block IP iptables -A INPUT -s BLOCKED_IP -j DROP # block IP/port iptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP # allow IP (insert at the bottom) iptables -A INPUT -s ALLOWED_IP -j ACCEPT # allow IP (insert at the top) iptables -I INPUT -s ALLOWED_IP -j ACCEPT # allow IP (specify order) iptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT # delete rule #number in chain iptables -D INPUT RULE_NUMBER # flush any existing NAT rules iptables -t nat -F # source NAT (SNAT) or masquerading: changes the IP source address information iptables -t nat -A POSTROUTING -o INTNAME -j SNAT --to INTIP iptables -t nat -A POSTROUTING -o INTNAME -j MASQUERADE # destination NAT (DNAT): changes the destination address iptables -t nat -A PREROUTING -d ORIGIP -j DNAT --to-destination NEWIP iptables -t nat -A PREROUTING -p PROTO -d ORIGIP --dport ORIGPORT -j DNAT --to-destination NEWIP:NEWPORT # show ARP kernel cache arp -n # list wireless network iw dev NETWORK_INTERFACE scan # show details of connected network iw dev NETWORK_INTERFACE link # manage both authentication and encryption for a wireless network interface wpa_supplicant Active network traffic capture Port-Forwarding proxy SOCKS proxy (preferred) # specify a SOCKS proxy for any TCP connection java \u2013DsocksProxyHost=localhost \u2013DsocksProxyPort=1080 XXX HTTP forwarding proxy (client side) HTTP reverse proxy (server side) Resources Subnetting dsniff Netfilter Dante - A free SOCKS server Secure Socket Funneling Iptables Essentials iptables vs nftables","title":"Network"},{"location":"linux/#applications","text":"# old insecure telnet www.wikipedia.org 80 # press enter twice after GET / HTTP/1.0 # details about communication curl --trace-ascii trace_file https://www.wikipedia.org > /dev/null vim trace_file # sshd server configs vim /etc/ssh/sshd_config # generate key pair ssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH # list network interfaces tcpdump -D # sniff hex and ascii (-A) by interface/host/port tcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER # Swiss Army knife # banner grabbing cat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER # install traditional (with -e option) apt-get install netcat -y # choose /bin/nc.traditional update-alternatives --config nc # listen on server netcat -l -p 6996 -e /bin/bash # run client cat <(echo ls -la) - | netcat IP_ADDRESS 6996 # simplest fuzz test cat /dev/urandom | nc IP_ADDRESS PORT_NUMBER # scan open ports nmap -Pn IP_ADDRESS # other # * ssh/scp/sftp/rsync # * curl/wget # system calls man recv man send # DHCP is designed to run on IP networks to distribute network configuration # information to nodes automatically: # IP address, default gateway, routing tables, default DNS servers, etc... # ARP finds the Ethernet address for a given IP address # * DHCP Spoofing # * ARP Poisoning ettercap -G # cli debuggers # CDB on Windows, GDB on Linux, and LLDB on macOS Resources Wireshark tcpdump tshark Nmap Ettercap Service Name and Transport Protocol Port Number Registry TODO add more examples","title":"Applications"},{"location":"linux/#shellterminal","text":"Fish Oil Zellij","title":"Shell/Terminal"},{"location":"linux/#blogs","text":"Highly useful Linux commands & configurations","title":"Blogs"},{"location":"linux/#other-resources","text":"OpenWrt BusyBox X.509 RFC OpenSSL Command-Line HOWTO Program Library HOWTO Shorewall Postfix HTTPie jq Lynx Samba","title":"Other resources"},{"location":"operating-system/","text":"Operating System Courses 6.033: Computer System Engineering MIT 6.S081: Operating System Engineering Xv6: A simple Unix-like teaching operating system CS 377: Operating Systems (youtube) Operating System (youtube) CS 422/522: Design and Implementation of Operating Systems CS 3210: Build an operating system in Rust programming language on Raspberry Pi 3 ITSC 3181: Introduction to Computer Architecture LFD103: A Beginner's Guide to Linux Kernel Development Books Operating System Concepts (10th) [ book | slides ] Lion's Commentary on UNIX with Source Code UNIX Internals: The New Frontiers Linux From Scratch (online) The little book about OS development OS01: Bootstrap yourself to write an OS from scratch (incomplete) Learning operating system development using Linux kernel and Raspberry Pi (incomplete) Writing a \"bare metal\" operating system for Raspberry Pi 4 Writing a simple 16 bit VM in less than 125 lines of C os-tutorial: How to create an OS from scratch Kernel The Linux Kernel Archives OldLinux: Early Linux Kernel Analysis and Comments Writing Your First Kernel Module Biscuit: An OS kernel in a high-level language HermiTux: A binary-compatible unikernel The big idea around unikernels State of the art for Unikernels Tiny Core Linux Boot Bootloader basics Interactive x86 bootloader A set of minimal dependency bootstrap binaries Writing an x86 bootloader in Rust that can launch vmlinux Rust Writing an OS in Rust Rust OS comparison Kerla: A new operating system kernel with Linux binary compatibility written in Rust Redox: Redox is a Unix-like Operating System written in Rust Operating System development tutorials in Rust on the Raspberry Pi CrabOS: My hobby operating system written in Rust (hobby) SnakeOS: Bootable x86 snake game in rust (hobby) Hobby SerenityOS Chicago95 ToaruOS MenuetOS oasis Alternative Nanos Qubes OS Random Awesome Operating System Stuff QEMU: A generic and open source machine emulator and virtualizer Hypervisor From Scratch SCAMP CPU - A homebrew 16-bit CPU with a homebrew Unix-like-ish operating system Virtual Hackintosh How To Write a Computer Emulator Linux x86 Program Start Up Floppinux: An Embedded Linux on a Single Floppy","title":"Operating System"},{"location":"operating-system/#operating-system","text":"","title":"Operating System"},{"location":"operating-system/#courses","text":"6.033: Computer System Engineering MIT 6.S081: Operating System Engineering Xv6: A simple Unix-like teaching operating system CS 377: Operating Systems (youtube) Operating System (youtube) CS 422/522: Design and Implementation of Operating Systems CS 3210: Build an operating system in Rust programming language on Raspberry Pi 3 ITSC 3181: Introduction to Computer Architecture LFD103: A Beginner's Guide to Linux Kernel Development","title":"Courses"},{"location":"operating-system/#books","text":"Operating System Concepts (10th) [ book | slides ] Lion's Commentary on UNIX with Source Code UNIX Internals: The New Frontiers Linux From Scratch (online) The little book about OS development OS01: Bootstrap yourself to write an OS from scratch (incomplete) Learning operating system development using Linux kernel and Raspberry Pi (incomplete) Writing a \"bare metal\" operating system for Raspberry Pi 4 Writing a simple 16 bit VM in less than 125 lines of C os-tutorial: How to create an OS from scratch","title":"Books"},{"location":"operating-system/#kernel","text":"The Linux Kernel Archives OldLinux: Early Linux Kernel Analysis and Comments Writing Your First Kernel Module Biscuit: An OS kernel in a high-level language HermiTux: A binary-compatible unikernel The big idea around unikernels State of the art for Unikernels Tiny Core Linux","title":"Kernel"},{"location":"operating-system/#boot","text":"Bootloader basics Interactive x86 bootloader A set of minimal dependency bootstrap binaries Writing an x86 bootloader in Rust that can launch vmlinux","title":"Boot"},{"location":"operating-system/#rust","text":"Writing an OS in Rust Rust OS comparison Kerla: A new operating system kernel with Linux binary compatibility written in Rust Redox: Redox is a Unix-like Operating System written in Rust Operating System development tutorials in Rust on the Raspberry Pi CrabOS: My hobby operating system written in Rust (hobby) SnakeOS: Bootable x86 snake game in rust (hobby)","title":"Rust"},{"location":"operating-system/#hobby","text":"SerenityOS Chicago95 ToaruOS MenuetOS oasis","title":"Hobby"},{"location":"operating-system/#alternative","text":"Nanos Qubes OS","title":"Alternative"},{"location":"operating-system/#random","text":"Awesome Operating System Stuff QEMU: A generic and open source machine emulator and virtualizer Hypervisor From Scratch SCAMP CPU - A homebrew 16-bit CPU with a homebrew Unix-like-ish operating system Virtual Hackintosh How To Write a Computer Emulator Linux x86 Program Start Up Floppinux: An Embedded Linux on a Single Floppy","title":"Random"},{"location":"other-resources/","text":"Other resources Computer Science CS 101: Introduction to Computing Principles Stanford CS Education Library Foundations of Computer Science Computer Networks From Scratch Code With Engineering Playbook Which programs are faster? Addison-Wesley Professional Computing Series Machine Learning Machine Learning (Course) Machine Learning Crash Course (Course) Amazon's Machine Learning University (Course) Making Things Think (Book) Machine Learning from Scratch (Book) What is Natural Language Processing? Scipy Lecture Notes Neural Networks An Introduction to Tensor Calculus Neural Network From Scratch The latest in Machine Learning (Papers) Book collections The Online Books Page A collection of free books from Springer E-Books Directory OpenStax Mark Watson: author of 20+ books LibriVox (audiobook) Textbooks Global Grey Pirate Library Mirror freeread.org: For the human right to read Random suckless Biohacking Lite Hacker News Ask HN: Great Blogs by Programmers Ask HN: Can I see your cheatsheet? Ask HN: What are the major open source alternatives to Auth0?","title":"Other Resources"},{"location":"other-resources/#other-resources","text":"","title":"Other resources"},{"location":"other-resources/#computer-science","text":"CS 101: Introduction to Computing Principles Stanford CS Education Library Foundations of Computer Science Computer Networks From Scratch Code With Engineering Playbook Which programs are faster? Addison-Wesley Professional Computing Series","title":"Computer Science"},{"location":"other-resources/#machine-learning","text":"Machine Learning (Course) Machine Learning Crash Course (Course) Amazon's Machine Learning University (Course) Making Things Think (Book) Machine Learning from Scratch (Book) What is Natural Language Processing? Scipy Lecture Notes Neural Networks An Introduction to Tensor Calculus Neural Network From Scratch The latest in Machine Learning (Papers)","title":"Machine Learning"},{"location":"other-resources/#book-collections","text":"The Online Books Page A collection of free books from Springer E-Books Directory OpenStax Mark Watson: author of 20+ books LibriVox (audiobook) Textbooks Global Grey Pirate Library Mirror freeread.org: For the human right to read","title":"Book collections"},{"location":"other-resources/#random","text":"suckless Biohacking Lite","title":"Random"},{"location":"other-resources/#hacker-news","text":"Ask HN: Great Blogs by Programmers Ask HN: Can I see your cheatsheet? Ask HN: What are the major open source alternatives to Auth0?","title":"Hacker News"},{"location":"programming/","text":"Programming Courses 6.001: Structure and Interpretation of Computer Programs MIT [ course | book ] CS 6120: Advanced Compilers History of Programming Languages Books Compilers: Principles, Techniques, and Tools Writing an Interpreter and Compiler in Go Crafting Interpreters (online) Programming Languages: Application and Interpretation (online) Build Your Own Lisp (online) LispE: Lisp El\u00e9mentaire (online) Interpreter / Compiler awesome-compilers Compiler Explorer Let's Build a Compiler A C version of the \"Let's Build a Compiler\" Let's write a compiler An Intro to Compilers (archive) Tiny C Compiler rui314/chibicc: A small C compiler The Super Tiny Compiler! Obfuscated Tiny C Compiler Tinylisp: Lisp in 99 lines of C and how to write one yourself Lessons from Writing a Compiler Parser Parsing Text with Nom How to write a tree-sitter grammar in an afternoon Writing a Simple Parser in Rust LLVM How to learn compilers: LLVM edition A Complete Guide to LLVM for Programming Language Creators Random Esolang Compile code into silicon Make A Language (rust) Designing a programming language mirdaki/theforce: The Force - A Star Wars themed programming language riicchhaarrd/ocean: Programming language that compiles into a x86 ELF executable adam-mcdaniel/oakc: An infinitely more portable alternative to the C programming language Creating the Golfcart Programming Language I wrote a linker everyone can understand! spencertipping/jit-tutorial: How to write a JIT compiler What Every Computer Scientist Should Know About Floating-Point Arithmetic","title":"Programming"},{"location":"programming/#programming","text":"","title":"Programming"},{"location":"programming/#courses","text":"6.001: Structure and Interpretation of Computer Programs MIT [ course | book ] CS 6120: Advanced Compilers History of Programming Languages","title":"Courses"},{"location":"programming/#books","text":"Compilers: Principles, Techniques, and Tools Writing an Interpreter and Compiler in Go Crafting Interpreters (online) Programming Languages: Application and Interpretation (online) Build Your Own Lisp (online) LispE: Lisp El\u00e9mentaire (online)","title":"Books"},{"location":"programming/#interpreter-compiler","text":"awesome-compilers Compiler Explorer Let's Build a Compiler A C version of the \"Let's Build a Compiler\" Let's write a compiler An Intro to Compilers (archive) Tiny C Compiler rui314/chibicc: A small C compiler The Super Tiny Compiler! Obfuscated Tiny C Compiler Tinylisp: Lisp in 99 lines of C and how to write one yourself Lessons from Writing a Compiler","title":"Interpreter / Compiler"},{"location":"programming/#parser","text":"Parsing Text with Nom How to write a tree-sitter grammar in an afternoon Writing a Simple Parser in Rust","title":"Parser"},{"location":"programming/#llvm","text":"How to learn compilers: LLVM edition A Complete Guide to LLVM for Programming Language Creators","title":"LLVM"},{"location":"programming/#random","text":"Esolang Compile code into silicon Make A Language (rust) Designing a programming language mirdaki/theforce: The Force - A Star Wars themed programming language riicchhaarrd/ocean: Programming language that compiles into a x86 ELF executable adam-mcdaniel/oakc: An infinitely more portable alternative to the C programming language Creating the Golfcart Programming Language I wrote a linker everyone can understand! spencertipping/jit-tutorial: How to write a JIT compiler What Every Computer Scientist Should Know About Floating-Point Arithmetic","title":"Random"},{"location":"scala/","text":"Scala Moved to scala-fp","title":"Scala (OLD)"},{"location":"scala/#scala","text":"Moved to scala-fp","title":"Scala"},{"location":"system-design/","text":"System Design Books Designing Data-Intensive Applications (2017) by Martin Kleppmann Domain-Driven Design: Tackling Complexity in the Heart of Software (2003) by Eric Evans Functional and Reactive Domain Modeling (2016) by Debasish Ghosh Versioning in an Event Sourced System Exploring CQRS and Event Sourcing Database Internals - A Deep Dive into How Distributed Data Systems Work The Architecture of Open Source Applications (free) Resources 6.824 Distributed Systems MIT (course) Distributed Systems lecture series by Martin Kleppmann (course) Software Architecture Monday (videos) CQRS by Martin Fowler Clarified CQRS 1 Year of Event Sourcing and CQRS Eventually Consistent - Revisited How do CRDTs solve distributed data consistency challenges? Are CRDTs suitable for shared editing? On Designing and Deploying Internet-Scale Services There is No Now Online Event Processing The world beyond batch: Streaming 101 Questioning the Lambda Architecture The Difference between SLI, SLO, and SLA A review of consensus protocols How you could have come up with Paxos yourself Implementing Raft's Leader Election in Rust Consensus Protocol Implementing Raft for Browsers with Rust and WebRTC HTTP Feeds Autopilot Pattern Applications REST Hooks Blogs Jepsen The Paper Trail High Scalability InfoQ: Architecture & Design Content CAP Brewer's CAP Theorem CAP Twelve Years Later: How the \"Rules\" Have Changed Please stop calling databases CP or AP The CAP FAQ You Can't Sacrifice Partition Tolerance Papers Foundational distributed systems papers (collection) Distributed Systems Reading List (collection) Best Paper Awards in Computer Science (collection) Ask HN: Recommended books and papers on distributed systems? (collection) The Google File System MapReduce: Simplified Data Processing on Large Clusters Raft: In Search of an Understandable Consensus Algorithm Paxos Made Simple Zab: A simple totally ordered broadcast protocol The Chubby lock service for loosely-coupled distributed systems Spanner: Google's Globally-Distributed Database Dynamo: Amazon\u2019s Highly Available Key-value Store HyperLogLog in Practice Dapper, a Large-Scale Distributed Systems Tracing Infrastructure Large-scale cluster management at Google with Borg Linearizability: A Correctness Condition for Concurrent Objects Harvest, Yield, and Scalable Tolerant Systems Life beyond Distributed Transactions (webarchive) The \u03d5 Accrual Failure Detector (webarchive) Conflict-free Replicated Data Types FLP - Impossibility of Distributed Consensus with One Faulty Process (webarchive) SEDA: An Architecture for Well-Conditioned, Scalable Internet Services Pregel: A System for Large-Scale Graph Processing Hashed and Hierarchical Timing Wheels Merkle Hash Tree based Techniques for Data Integrity of Outsourced Data What Every Programmer Should Know About Memory Fallacies of Distributed Computing Explained (webarchive) The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing A Dataset of Dockerfiles TODO acronyms commands vs events OLEP online event processing OLTP online transaction processing OLAP online analytical processing DDD CQRS Event Source Eventual Consistency CRDT CAP theorem two-phase commit circuit breaker pattern SAGA SLA service-level agreement ACID ACID 2.0 Associative grouping doesn't matter a+(b+c)=(a+b)+c Commutative order doesn't matter a+b=b+a Idempotent duplication doesn't matter a+a=a -->","title":"System Design"},{"location":"system-design/#system-design","text":"","title":"System Design"},{"location":"system-design/#books","text":"Designing Data-Intensive Applications (2017) by Martin Kleppmann Domain-Driven Design: Tackling Complexity in the Heart of Software (2003) by Eric Evans Functional and Reactive Domain Modeling (2016) by Debasish Ghosh Versioning in an Event Sourced System Exploring CQRS and Event Sourcing Database Internals - A Deep Dive into How Distributed Data Systems Work The Architecture of Open Source Applications (free)","title":"Books"},{"location":"system-design/#resources","text":"6.824 Distributed Systems MIT (course) Distributed Systems lecture series by Martin Kleppmann (course) Software Architecture Monday (videos) CQRS by Martin Fowler Clarified CQRS 1 Year of Event Sourcing and CQRS Eventually Consistent - Revisited How do CRDTs solve distributed data consistency challenges? Are CRDTs suitable for shared editing? On Designing and Deploying Internet-Scale Services There is No Now Online Event Processing The world beyond batch: Streaming 101 Questioning the Lambda Architecture The Difference between SLI, SLO, and SLA A review of consensus protocols How you could have come up with Paxos yourself Implementing Raft's Leader Election in Rust Consensus Protocol Implementing Raft for Browsers with Rust and WebRTC HTTP Feeds Autopilot Pattern Applications REST Hooks","title":"Resources"},{"location":"system-design/#blogs","text":"Jepsen The Paper Trail High Scalability InfoQ: Architecture & Design Content","title":"Blogs"},{"location":"system-design/#cap","text":"Brewer's CAP Theorem CAP Twelve Years Later: How the \"Rules\" Have Changed Please stop calling databases CP or AP The CAP FAQ You Can't Sacrifice Partition Tolerance","title":"CAP"},{"location":"system-design/#papers","text":"Foundational distributed systems papers (collection) Distributed Systems Reading List (collection) Best Paper Awards in Computer Science (collection) Ask HN: Recommended books and papers on distributed systems? (collection) The Google File System MapReduce: Simplified Data Processing on Large Clusters Raft: In Search of an Understandable Consensus Algorithm Paxos Made Simple Zab: A simple totally ordered broadcast protocol The Chubby lock service for loosely-coupled distributed systems Spanner: Google's Globally-Distributed Database Dynamo: Amazon\u2019s Highly Available Key-value Store HyperLogLog in Practice Dapper, a Large-Scale Distributed Systems Tracing Infrastructure Large-scale cluster management at Google with Borg Linearizability: A Correctness Condition for Concurrent Objects Harvest, Yield, and Scalable Tolerant Systems Life beyond Distributed Transactions (webarchive) The \u03d5 Accrual Failure Detector (webarchive) Conflict-free Replicated Data Types FLP - Impossibility of Distributed Consensus with One Faulty Process (webarchive) SEDA: An Architecture for Well-Conditioned, Scalable Internet Services Pregel: A System for Large-Scale Graph Processing Hashed and Hierarchical Timing Wheels Merkle Hash Tree based Techniques for Data Integrity of Outsourced Data What Every Programmer Should Know About Memory Fallacies of Distributed Computing Explained (webarchive) The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing A Dataset of Dockerfiles TODO acronyms commands vs events OLEP online event processing OLTP online transaction processing OLAP online analytical processing DDD CQRS Event Source Eventual Consistency CRDT CAP theorem two-phase commit circuit breaker pattern SAGA SLA service-level agreement ACID ACID 2.0 Associative grouping doesn't matter a+(b+c)=(a+b)+c Commutative order doesn't matter a+b=b+a Idempotent duplication doesn't matter a+a=a -->","title":"Papers"},{"location":"toolbox/","text":"Toolbox Vagrant Vagrant is a tool for building and managing virtual machine environments in a single workflow Resources Documentation VirtualBox Setup project creating a Vagrantfile vagrant init Boot and connect to the default virtual machine vagrant up vagrant status vagrant ssh Useful commands # shut down gracefully vagrant halt # reload (halt + up) + re-provision vagrant reload --provision # update box vagrant box update vagrant box list # delete virtual machine without prompt vagrant destory -f MkDocs MkDocs is a static site generator Resources Documentation Install pip install mkdocs sudo -H pip3 install mkdocs Useful commands # setup in current directory mkdocs new . # start dev server with hot reload @ http://127.0.0.1:8000 mkdocs serve # build static site mkdocs build --clean # deploy to github mkdocs gh-deploy Hugo Hugo is a static site generator Documentation Useful commands # install snap install hugo # switch to extended Sass/SCSS version snap refresh hugo --channel=extended # create skeleton hugo new site docs # create skeleton in current non-empty folder hugo new site . --force # setup theme git submodule add https://github.com/alex-shpak/hugo-book themes/book echo 'theme = \"book\"' >> config.toml # start dev server hugo server -D SDKMAN! SDKMAN! is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems Resources Documentation Setup curl -s \"https://get.sdkman.io\" | bash source \"$HOME/.sdkman/bin/sdkman-init.sh\" sdk version Gradle # setup sdk list gradle sdk install gradle 4.4.1 gradle -version # create Gradle project mkdir -p PROJECT_NAME && cd $_ gradle init --type java-library ./gradlew clean build Scala # setup sbt sdk list sbt sdk install sbt sbt sbtVersion sbt about # setup scala sdk list scala sdk install scala 2.11.8 scala -version # sample project sbt new sbt/scala-seed.g8 Giter8 Giter8 is a command line tool to generate files and directories from templates published on GitHub or any other git repository Resources Documentation Templates Setup # install conscript curl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh source ~/.bashrc # install g8 cs foundweekends/giter8 Example # interactive g8 sbt/scala-seed.g8 # non-interactive g8 sbt/scala-seed.g8 --name=my-new-website Snap Resources Documentation Useful commands # search snap find gimp # info snap info gimp # install snap install gimp # list installed app snap list # update all packages snap refresh # remove snap remove gimp Python Resources pip virtualenv What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ? Setup # search apt-get update && apt-cache search python | grep python2 # setup python apt-get install -y python2.7 apt-get install -y python3 # install pip + setuptools curl https://bootstrap.pypa.io/get-pip.py | python2.7 - curl https://bootstrap.pypa.io/get-pip.py | python3 - apt install -y python-pip apt install -y python3-pip # upgrade pip pip install -U pip # install virtualenv globally pip install virtualenv virtualenv # create virtualenv virtualenv venv virtualenv -p python3 venv virtualenv -p $(which python3) venv # activate virtualenv source venv/bin/activate # verify virtualenv which python python --version # deactivate virtualenv deactivate pip # search package pip search <package> # install new package pip install <package> # update requirements with new packages pip freeze > requirements.txt # install all requirements pip install -r requirements.txt Other # generate rc file pylint --generate-rcfile > .pylintrc # create module touch app/{__init__,main}.py Git Resources git - the simple guide git notes (1) git notes (2) Other Oh Shit, Git!?! Using Askgit git filter-repo is a versatile tool for rewriting history Merkle Tree The Myers diff algorithm Mercurial Resources A Guide to Branching in Mercurial # changes since last commit hg st # verify current branch hg branch # lists all branches hg branches # checkout default branch hg up default # pull latest changes hg pull -u # create new branch hg branch \"branch-name\" # track new file hg add . # track new files and untrack removed files hg addremove # commit all tracked files hg commit -m \"my-comment\" # commit specific files hg commit FILE_1 FILE_2 -m \"my-comment\" # commit and track/untrack files (i.e. addremove) hg commit -A -m \"my-comment-with-addremove\" # rename last unpushed commit message hg commit -m \"bad-commit-message\" hg commit --amend -m \"good-commit-message\" # discard untracked files hg purge # discard uncommitted local changes hg up -C # discard local uncommitted branch hg strip \"branch-name\" # push commits in all branches hg push # push commits in current branch hg push -b . # create a new branch and push commits in current branch (first time only) hg push -b . --new-branch # lists unpushed commit hg outgoing # change head to specific revision hg up -r 12345 # merge default branch on current branch hg up default hg pull -u hg status hg up CURRENT-BRANCH hg merge default hg diff # remove all resolved conflicts rm **/*.orig # list stashes hg shelve --list # stash hg shelve -n \"my-draft\" # unstash hg unshelve \"my-draft\" # revert/undo last unpushed commit hg strip -r -1 --keep hg strip --keep --rev . # solve conflicts manually and then mark it as merged hg resolve -m FILE-NAME # lists commits hg log hg ls # pretty log hg history --graph --limit 10","title":"Toolbox"},{"location":"toolbox/#toolbox","text":"","title":"Toolbox"},{"location":"toolbox/#vagrant","text":"Vagrant is a tool for building and managing virtual machine environments in a single workflow Resources Documentation VirtualBox Setup project creating a Vagrantfile vagrant init Boot and connect to the default virtual machine vagrant up vagrant status vagrant ssh Useful commands # shut down gracefully vagrant halt # reload (halt + up) + re-provision vagrant reload --provision # update box vagrant box update vagrant box list # delete virtual machine without prompt vagrant destory -f","title":"Vagrant"},{"location":"toolbox/#mkdocs","text":"MkDocs is a static site generator Resources Documentation Install pip install mkdocs sudo -H pip3 install mkdocs Useful commands # setup in current directory mkdocs new . # start dev server with hot reload @ http://127.0.0.1:8000 mkdocs serve # build static site mkdocs build --clean # deploy to github mkdocs gh-deploy","title":"MkDocs"},{"location":"toolbox/#hugo","text":"Hugo is a static site generator Documentation Useful commands # install snap install hugo # switch to extended Sass/SCSS version snap refresh hugo --channel=extended # create skeleton hugo new site docs # create skeleton in current non-empty folder hugo new site . --force # setup theme git submodule add https://github.com/alex-shpak/hugo-book themes/book echo 'theme = \"book\"' >> config.toml # start dev server hugo server -D","title":"Hugo"},{"location":"toolbox/#sdkman","text":"SDKMAN! is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems Resources Documentation Setup curl -s \"https://get.sdkman.io\" | bash source \"$HOME/.sdkman/bin/sdkman-init.sh\" sdk version Gradle # setup sdk list gradle sdk install gradle 4.4.1 gradle -version # create Gradle project mkdir -p PROJECT_NAME && cd $_ gradle init --type java-library ./gradlew clean build Scala # setup sbt sdk list sbt sdk install sbt sbt sbtVersion sbt about # setup scala sdk list scala sdk install scala 2.11.8 scala -version # sample project sbt new sbt/scala-seed.g8","title":"SDKMAN!"},{"location":"toolbox/#giter8","text":"Giter8 is a command line tool to generate files and directories from templates published on GitHub or any other git repository Resources Documentation Templates Setup # install conscript curl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh source ~/.bashrc # install g8 cs foundweekends/giter8 Example # interactive g8 sbt/scala-seed.g8 # non-interactive g8 sbt/scala-seed.g8 --name=my-new-website","title":"Giter8"},{"location":"toolbox/#snap","text":"Resources Documentation Useful commands # search snap find gimp # info snap info gimp # install snap install gimp # list installed app snap list # update all packages snap refresh # remove snap remove gimp","title":"Snap"},{"location":"toolbox/#python","text":"Resources pip virtualenv What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ? Setup # search apt-get update && apt-cache search python | grep python2 # setup python apt-get install -y python2.7 apt-get install -y python3 # install pip + setuptools curl https://bootstrap.pypa.io/get-pip.py | python2.7 - curl https://bootstrap.pypa.io/get-pip.py | python3 - apt install -y python-pip apt install -y python3-pip # upgrade pip pip install -U pip # install virtualenv globally pip install virtualenv virtualenv # create virtualenv virtualenv venv virtualenv -p python3 venv virtualenv -p $(which python3) venv # activate virtualenv source venv/bin/activate # verify virtualenv which python python --version # deactivate virtualenv deactivate pip # search package pip search <package> # install new package pip install <package> # update requirements with new packages pip freeze > requirements.txt # install all requirements pip install -r requirements.txt Other # generate rc file pylint --generate-rcfile > .pylintrc # create module touch app/{__init__,main}.py","title":"Python"},{"location":"toolbox/#git","text":"Resources git - the simple guide git notes (1) git notes (2) Other Oh Shit, Git!?! Using Askgit git filter-repo is a versatile tool for rewriting history Merkle Tree The Myers diff algorithm","title":"Git"},{"location":"toolbox/#mercurial","text":"Resources A Guide to Branching in Mercurial # changes since last commit hg st # verify current branch hg branch # lists all branches hg branches # checkout default branch hg up default # pull latest changes hg pull -u # create new branch hg branch \"branch-name\" # track new file hg add . # track new files and untrack removed files hg addremove # commit all tracked files hg commit -m \"my-comment\" # commit specific files hg commit FILE_1 FILE_2 -m \"my-comment\" # commit and track/untrack files (i.e. addremove) hg commit -A -m \"my-comment-with-addremove\" # rename last unpushed commit message hg commit -m \"bad-commit-message\" hg commit --amend -m \"good-commit-message\" # discard untracked files hg purge # discard uncommitted local changes hg up -C # discard local uncommitted branch hg strip \"branch-name\" # push commits in all branches hg push # push commits in current branch hg push -b . # create a new branch and push commits in current branch (first time only) hg push -b . --new-branch # lists unpushed commit hg outgoing # change head to specific revision hg up -r 12345 # merge default branch on current branch hg up default hg pull -u hg status hg up CURRENT-BRANCH hg merge default hg diff # remove all resolved conflicts rm **/*.orig # list stashes hg shelve --list # stash hg shelve -n \"my-draft\" # unstash hg unshelve \"my-draft\" # revert/undo last unpushed commit hg strip -r -1 --keep hg strip --keep --rev . # solve conflicts manually and then mark it as merged hg resolve -m FILE-NAME # lists commits hg log hg ls # pretty log hg history --graph --limit 10","title":"Mercurial"},{"location":"zookeeper/","text":"ZooKeeper ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services Resources Documentation Curator Setup Requirements Base image Build devops/zookeeper image # change path cd devops/zookeeper # build image docker build -t devops/zookeeper:latest . # build image with specific version - see Dockerfile for version 3.5.x docker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 . # temporary container [host:container] docker run --rm --name zookeeper -p 12181:2181 devops/zookeeper # access container docker exec -it zookeeper bash # paths /opt/zookeeper /var/log/zookeeper /var/lib/zookeeper /var/log/supervisord.log # logs tail -F /var/log/supervisord.log # check service status supervisorctl status supervisorctl restart zookeeper Example docker exec -it zookeeper bash # (option 1) check zookeeper status echo ruok | nc localhost 2181 # (option 2) check zookeeper status telnet localhost 2181 # expect answer imok > ruok zkCli.sh -server 127.0.0.1:2181 help # list znodes ls / # create znode and associate value create /zk_test my_data # verify data get /zk_test # change value set /zk_test junk # delete znode delete /zk_test The four-letter words Category Command Description Server status ruok Prints imok if the server is running and not in an error state conf Prints the server configuration (from zoo.cfg) envi Prints the server environment, including ZooKeeper version, Java version, and other system properties srvr Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower) stat Prints server statistics and connected clients srst Resets server statistics isro Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw) Client connections dump Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command cons Lists connection statistics for all the server's clients crst Resets connection statistics Watches wchs Lists summary information for the server's watches wchc Lists all the server's watches by connection, may impact server performance for a large number of watches wchp Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches Monitoring mntr Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios","title":"ZooKeeper"},{"location":"zookeeper/#zookeeper","text":"ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services Resources Documentation Curator","title":"ZooKeeper"},{"location":"zookeeper/#setup","text":"Requirements Base image Build devops/zookeeper image # change path cd devops/zookeeper # build image docker build -t devops/zookeeper:latest . # build image with specific version - see Dockerfile for version 3.5.x docker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 . # temporary container [host:container] docker run --rm --name zookeeper -p 12181:2181 devops/zookeeper # access container docker exec -it zookeeper bash # paths /opt/zookeeper /var/log/zookeeper /var/lib/zookeeper /var/log/supervisord.log # logs tail -F /var/log/supervisord.log # check service status supervisorctl status supervisorctl restart zookeeper Example docker exec -it zookeeper bash # (option 1) check zookeeper status echo ruok | nc localhost 2181 # (option 2) check zookeeper status telnet localhost 2181 # expect answer imok > ruok zkCli.sh -server 127.0.0.1:2181 help # list znodes ls / # create znode and associate value create /zk_test my_data # verify data get /zk_test # change value set /zk_test junk # delete znode delete /zk_test","title":"Setup"},{"location":"zookeeper/#the-four-letter-words","text":"Category Command Description Server status ruok Prints imok if the server is running and not in an error state conf Prints the server configuration (from zoo.cfg) envi Prints the server environment, including ZooKeeper version, Java version, and other system properties srvr Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower) stat Prints server statistics and connected clients srst Resets server statistics isro Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw) Client connections dump Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command cons Lists connection statistics for all the server's clients crst Resets connection statistics Watches wchs Lists summary information for the server's watches wchc Lists all the server's watches by connection, may impact server performance for a large number of watches wchp Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches Monitoring mntr Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios","title":"The four-letter words"}]}